{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26ba668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import casadi as ca\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "\n",
    "from scipy.linalg import solve_continuous_are\n",
    "\n",
    "from ocslc.switched_linear_mpc import SwiLin\n",
    "from optimizers.sgd import StochasticGradientDescent, RMSPropOptimizer, AdamOptimizer\n",
    "from optimizers.sgd import sgd_optimize, rmsprop_optimize, adam_optimize\n",
    "\n",
    "def switched_problem(n_phases=5):\n",
    "    \"\"\"\n",
    "    Set up a switched linear problem and compute cost and gradient functions\n",
    "    \"\"\"\n",
    "    model = {\n",
    "        'A': [\n",
    "            np.array([[-2.5, 0.5, 0.3], [0.4, -2.0, 0.6], [0.2, 0.3, -1.8]]),\n",
    "            np.array([[-1.9, 3.2, 0.4], [0.3, -2.1, 0.5], [0, 0.6, -2.3]]),\n",
    "            np.array([[-2.2, 0, 0.5],   [0.2, -1.7, 0.4], [0.3, 0.2, -2.0]]),\n",
    "            np.array([[-1.8, 0.3, 0.2], [0.5, -2.4, 0],   [0.4, 0, -2.2]]),\n",
    "            np.array([[-2.0, 0.4, 0],   [0.3, -2.2, 0.2], [0.5, 0.3, -1.9]]),\n",
    "            np.array([[-2.3, 0.2, 0.3], [0, -2.0, 0.4],   [0.2, 0.5, -2.1]]),\n",
    "            np.array([[-1.7, 0.5, 0.4], [0.2, -2.5, 0.3], [1.1, 0.2, -2.4]]),\n",
    "            np.array([[-2.1, 0.3, 0.2], [0.4, -1.9, 0.5], [0.3, 0.1, -2.0]]),\n",
    "            np.array([[-2.4, 0, 0.5],   [0.2, -2.3, 0.3], [0.4, 0.2, -1.8]]),\n",
    "            np.array([[-1.8, 0.4, 0.3], [0.5, -2.1, 0.2], [0.2, 3.1, -2.2]]),\n",
    "        ],\n",
    "        'B': [\n",
    "            np.array([[1.5, 0.3], [0.4, 1.2], [0.2, 0.8]]),\n",
    "            np.array([[1.2, 0.5], [0.3, 0.9], [0.4, 1.1]]),\n",
    "            np.array([[1.0, 0.4], [0.5, 1.3], [0.3, 0.7]]),\n",
    "            np.array([[1.4, 0.2], [0.6, 1.0], [0.1, 0.9]]),\n",
    "            np.array([[1.3, 0.1], [0.2, 1.4], [0.5, 0.6]]),\n",
    "            np.array([[1.1, 0.3], [0.4, 1.5], [0.2, 0.8]]),\n",
    "            np.array([[1.6, 0.2], [0.3, 1.1], [0.4, 0.7]]),\n",
    "            np.array([[1.0, 0.4], [0.5, 1.2], [0.3, 0.9]]),\n",
    "            np.array([[1.2, 0.5], [0.1, 1.3], [0.6, 0.8]]),\n",
    "            np.array([[1.4, 0.3], [0.2, 1.0], [0.5, 0.7]]),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    n_states = model['A'][0].shape[0]\n",
    "    n_inputs = model['B'][0].shape[1]\n",
    "\n",
    "    time_horizon = 10\n",
    "\n",
    "    x0 = np.array([2, -1, 5])\n",
    "    \n",
    "    xr = np.array([1, -3])\n",
    "    \n",
    "    swi_lin = SwiLin(\n",
    "        n_phases, \n",
    "        n_states,\n",
    "        n_inputs,\n",
    "        time_horizon, \n",
    "        auto=False, \n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    swi_lin.load_model(model)\n",
    "\n",
    "    Q = 10. * np.eye(n_states)\n",
    "    R = 10. * np.eye(n_inputs)\n",
    "    E = 1. * np.eye(n_states)\n",
    "\n",
    "    swi_lin.precompute_matrices(x0, Q, R, E)\n",
    "    x0 = np.append(x0, 1)  # augment with 1 for affine term\n",
    "    J_func = swi_lin.cost_function(R, x0)\n",
    "        \n",
    "    grad_J_u = []\n",
    "    grad_J_delta = []\n",
    "\n",
    "    for k in range(n_phases):\n",
    "        # Compute gradient of the cost\n",
    "        du, d_delta = swi_lin.grad_cost_function(k, R)\n",
    "        # print(f\"Length du: {len(du)}\")\n",
    "\n",
    "        grad_J_u += du\n",
    "        grad_J_delta.append(d_delta)\n",
    "\n",
    "    grad_J = ca.vertcat(*grad_J_delta, *grad_J_u)\n",
    "\n",
    "    # keep the original stacked forms if needed\n",
    "    grad_J_u = np.hstack(grad_J_u)\n",
    "    grad_J_delta = np.hstack(grad_J_delta)\n",
    "    \n",
    "    # Create a CasADi function for the gradient\n",
    "    # grad_J_func = ca.Function('grad_J', [*swi_lin.u, *swi_lin.delta], [grad_J])\n",
    "    \n",
    "    u_vec = ca.vertcat(*swi_lin.u)\n",
    "    delta_vec = ca.vertcat(*swi_lin.delta)\n",
    "\n",
    "    grad_J_func = ca.Function('grad_J', [u_vec, delta_vec], [grad_J])\n",
    "        \n",
    "    # Create wrapper functions for the optimizer\n",
    "    def cost_function(params, indices=None, data=None):\n",
    "        \"\"\"\n",
    "        params: 1D array (n_params,) or 2D array (batch_size, n_params)\n",
    "        indices: optional indices to select a minibatch from a 2D params array\n",
    "        Returns the scalar loss (averaged over minibatch if batch provided).\n",
    "        \"\"\"\n",
    "        params = np.asarray(params)\n",
    "        # From a single flattened params vector, unpack into controls and durations\n",
    "        u = params[:n_phases * n_inputs].reshape((n_phases, n_inputs)).tolist()\n",
    "        phases_duration = params[n_phases * n_inputs:].reshape((n_phases,)).tolist()\n",
    "        params_list = u + phases_duration\n",
    "        # Compute the cost function for a single example (no batch)\n",
    "        J = float(J_func(*params_list).full().item())\n",
    "        J = 0\n",
    "        \n",
    "        if data is not None:\n",
    "            u = data['controls'].ravel()\n",
    "            phases_duration = data['phases_duration'].ravel()\n",
    "            params_ref = np.concatenate([u, phases_duration])\n",
    "            # print(f\"Reference params: {params_ref}\")\n",
    "            \n",
    "            # Compute the loss wrt reference params\n",
    "            params_ref = np.asarray(params_ref)\n",
    "            # add numpy sum of squared differences to scalar loss\n",
    "            J += float(np.sum((params - params_ref) ** 2) / len(params_ref))\n",
    "\n",
    "        return J\n",
    "\n",
    "    def gradient_function(params, indices=None, data=None):\n",
    "        \"\"\"\n",
    "        params: 1D array (n_params,) or 2D array (batch_size, n_params)\n",
    "        indices: optional indices to select a minibatch from a 2D params array\n",
    "        Returns gradient vector (n_params,) averaged over minibatch if batch provided.\n",
    "        \"\"\"\n",
    "        params = np.asarray(params)\n",
    "        # From a single flattened params vector, unpack into controls and durations\n",
    "        u = params[:n_phases * n_inputs].reshape((n_phases, n_inputs)).tolist()\n",
    "        phases_duration = params[n_phases * n_inputs:].reshape((n_phases,)).tolist()\n",
    "        params_list = u + phases_duration\n",
    "        # single example\n",
    "        grad_J = np.asarray(grad_J_func(*params_list).full().ravel())\n",
    "        grad_J = 0\n",
    "\n",
    "        if data is not None:\n",
    "            u = data['controls'].ravel()\n",
    "            phases_duration = data['phases_duration'].ravel()\n",
    "            params_ref = np.concatenate([u, phases_duration])\n",
    "            params_ref = np.asarray(params_ref)\n",
    "            # add gradient of the squared differences to grad_J\n",
    "            grad_J = 2 * (params - params_ref) / len(params_ref)\n",
    "\n",
    "        return grad_J\n",
    "\n",
    "\n",
    "    return J_func, grad_J_func, cost_function, gradient_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cost and gradient functions from the switched problem\n",
    "J_func, grad_J_func, cost_function, gradient_function = switched_problem(n_phases=1)\n",
    "\n",
    "# Define range for u and delta (for single phase problem)\n",
    "n_phases = 1\n",
    "n_inputs = 2\n",
    "u1_values = np.linspace(-5, 5, 50)\n",
    "u2_values = np.linspace(-5, 5, 50)\n",
    "delta_values_switch = np.linspace(0.1, 2.0, 50)\n",
    "\n",
    "# Create 2D plots for cost function with respect to (u1, u2) at fixed delta\n",
    "U1, U2 = np.meshgrid(u1_values, u2_values)\n",
    "J_switch = np.zeros_like(U1)\n",
    "fixed_delta = 1.0\n",
    "\n",
    "for i in range(len(u2_values)):\n",
    "    for j in range(len(u1_values)):\n",
    "        u = [[u1_values[j], u2_values[i]]]\n",
    "        phases_duration = [fixed_delta]\n",
    "        J_switch[i, j] = float(J_func(*u, *phases_duration).full().item())\n",
    "\n",
    "# Create plots\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.plot_surface(U1, U2, J_switch, cmap='viridis')\n",
    "ax1.set_xlabel('u1')\n",
    "ax1.set_ylabel('u2')\n",
    "ax1.set_zlabel('J')\n",
    "ax1.set_title(f'Switched System Cost J(u1, u2) at δ={fixed_delta}')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contourf(U1, U2, J_switch, levels=20, cmap='viridis')\n",
    "ax2.set_xlabel('u1')\n",
    "ax2.set_ylabel('u2')\n",
    "ax2.set_title(f'Cost Function J(u1, u2) - Contour at δ={fixed_delta}')\n",
    "plt.colorbar(contour, ax=ax2)\n",
    "\n",
    "# Plot cost vs delta at fixed u\n",
    "ax3 = fig.add_subplot(133)\n",
    "J_vs_delta = np.zeros(len(delta_values_switch))\n",
    "fixed_u = [[0.0, 0.0]]\n",
    "for i, delta in enumerate(delta_values_switch):\n",
    "    J_vs_delta[i] = float(J_func(*fixed_u, delta).full().item())\n",
    "ax3.plot(delta_values_switch, J_vs_delta, 'b-', linewidth=2)\n",
    "ax3.set_xlabel('delta')\n",
    "ax3.set_ylabel('J')\n",
    "ax3.set_title('Cost Function J(δ) at u=[0, 0]')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Plot gradients\n",
    "fig2 = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Calculate gradients at fixed delta\n",
    "grad_u1 = np.zeros_like(U1)\n",
    "grad_u2 = np.zeros_like(U2)\n",
    "\n",
    "for i in range(len(u2_values)):\n",
    "    for j in range(len(u1_values)):\n",
    "        u = [[u1_values[j], u2_values[i]]]\n",
    "        phases_duration = [fixed_delta]\n",
    "        grad = np.array(grad_J_func(*u, *phases_duration).full()).ravel()\n",
    "        grad_u1[i, j] = grad[1]  # gradient wrt u1\n",
    "        grad_u2[i, j] = grad[2]  # gradient wrt u2\n",
    "\n",
    "# Plot gradient wrt u1\n",
    "ax4 = fig2.add_subplot(221, projection='3d')\n",
    "ax4.plot_surface(U1, U2, grad_u1, cmap='plasma')\n",
    "ax4.set_xlabel('u1')\n",
    "ax4.set_ylabel('u2')\n",
    "ax4.set_zlabel('∂J/∂u1')\n",
    "ax4.set_title(f'Gradient ∂J/∂u1 at δ={fixed_delta}')\n",
    "\n",
    "# Plot gradient wrt u2\n",
    "ax5 = fig2.add_subplot(222, projection='3d')\n",
    "ax5.plot_surface(U1, U2, grad_u2, cmap='coolwarm')\n",
    "ax5.set_xlabel('u1')\n",
    "ax5.set_ylabel('u2')\n",
    "ax5.set_zlabel('∂J/∂u2')\n",
    "ax5.set_title(f'Gradient ∂J/∂u2 at δ={fixed_delta}')\n",
    "\n",
    "# Contour plots\n",
    "ax6 = fig2.add_subplot(223)\n",
    "contour1 = ax6.contourf(U1, U2, grad_u1, levels=20, cmap='plasma')\n",
    "ax6.set_xlabel('u1')\n",
    "ax6.set_ylabel('u2')\n",
    "ax6.set_title(f'Gradient ∂J/∂u1 - Contour at δ={fixed_delta}')\n",
    "plt.colorbar(contour1, ax=ax6)\n",
    "\n",
    "ax7 = fig2.add_subplot(224)\n",
    "contour2 = ax7.contourf(U1, U2, grad_u2, levels=20, cmap='coolwarm')\n",
    "ax7.set_xlabel('u1')\n",
    "ax7.set_ylabel('u2')\n",
    "ax7.set_title(f'Gradient ∂J/∂u2 - Contour at δ={fixed_delta}')\n",
    "plt.colorbar(contour2, ax=ax7)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Plot gradient wrt delta\n",
    "grad_delta_vs_delta = np.zeros(len(delta_values_switch))\n",
    "for i, delta in enumerate(delta_values_switch):\n",
    "    grad = np.array(grad_J_func(*fixed_u, delta).full()).ravel()\n",
    "    grad_delta_vs_delta[i] = grad[0]  # gradient wrt delta\n",
    "\n",
    "fig3 = plt.figure(figsize=(10, 5))\n",
    "ax8 = fig3.add_subplot(111)\n",
    "ax8.plot(delta_values_switch, grad_delta_vs_delta, 'r-', linewidth=2)\n",
    "ax8.set_xlabel('delta')\n",
    "ax8.set_ylabel('∂J/∂δ')\n",
    "ax8.set_title('Gradient ∂J/∂δ at u=[0, 0]')\n",
    "ax8.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62da5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze gradient discontinuities\n",
    "fig_disc = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Calculate gradient magnitude\n",
    "grad_magnitude = np.sqrt(grad_u1**2 + grad_u2**2)\n",
    "\n",
    "# Calculate spatial derivatives to find discontinuities\n",
    "# Using central differences to detect abrupt changes\n",
    "du1_du1 = np.gradient(grad_u1, axis=1)\n",
    "du1_du2 = np.gradient(grad_u1, axis=0)\n",
    "du2_du1 = np.gradient(grad_u2, axis=1)\n",
    "du2_du2 = np.gradient(grad_u2, axis=0)\n",
    "\n",
    "# Total variation measure (high values indicate discontinuities)\n",
    "total_variation_u1 = np.abs(du1_du1) + np.abs(du1_du2)\n",
    "total_variation_u2 = np.abs(du2_du1) + np.abs(du2_du2)\n",
    "total_variation = total_variation_u1 + total_variation_u2\n",
    "\n",
    "# Plot 1: Gradient magnitude\n",
    "ax1_disc = fig_disc.add_subplot(231)\n",
    "im1 = ax1_disc.contourf(U1, U2, grad_magnitude, levels=30, cmap='viridis')\n",
    "ax1_disc.set_xlabel('u1')\n",
    "ax1_disc.set_ylabel('u2')\n",
    "ax1_disc.set_title('Gradient Magnitude')\n",
    "plt.colorbar(im1, ax=ax1_disc)\n",
    "\n",
    "# Plot 2: Total variation (discontinuity indicator)\n",
    "ax2_disc = fig_disc.add_subplot(232)\n",
    "im2 = ax2_disc.contourf(U1, U2, total_variation, levels=30, cmap='hot')\n",
    "ax2_disc.set_xlabel('u1')\n",
    "ax2_disc.set_ylabel('u2')\n",
    "ax2_disc.set_title('Total Variation (Discontinuity Indicator)')\n",
    "plt.colorbar(im2, ax=ax2_disc)\n",
    "\n",
    "# Plot 3: Highlight high discontinuity regions\n",
    "ax3_disc = fig_disc.add_subplot(233)\n",
    "threshold = np.percentile(total_variation, 90)  # Top 10% variations\n",
    "discontinuity_mask = total_variation > threshold\n",
    "ax3_disc.contourf(U1, U2, discontinuity_mask.astype(float), levels=[0, 0.5, 1], cmap='RdYlGn_r')\n",
    "ax3_disc.set_xlabel('u1')\n",
    "ax3_disc.set_ylabel('u2')\n",
    "ax3_disc.set_title(f'High Discontinuity Regions (>{threshold:.2f})')\n",
    "\n",
    "# Plot 4: Second derivative magnitude for ∂J/∂u1\n",
    "ax4_disc = fig_disc.add_subplot(234)\n",
    "im4 = ax4_disc.contourf(U1, U2, total_variation_u1, levels=30, cmap='plasma')\n",
    "ax4_disc.set_xlabel('u1')\n",
    "ax4_disc.set_ylabel('u2')\n",
    "ax4_disc.set_title('Variation in ∂J/∂u1')\n",
    "plt.colorbar(im4, ax=ax4_disc)\n",
    "\n",
    "# Plot 5: Second derivative magnitude for ∂J/∂u2\n",
    "ax5_disc = fig_disc.add_subplot(235)\n",
    "im5 = ax5_disc.contourf(U1, U2, total_variation_u2, levels=30, cmap='coolwarm')\n",
    "ax5_disc.set_xlabel('u1')\n",
    "ax5_disc.set_ylabel('u2')\n",
    "ax5_disc.set_title('Variation in ∂J/∂u2')\n",
    "plt.colorbar(im5, ax=ax5_disc)\n",
    "\n",
    "# Plot 6: Line plot showing gradient behavior along u1 at u2=0\n",
    "ax6_disc = fig_disc.add_subplot(236)\n",
    "u2_zero_idx = np.argmin(np.abs(u2_values))\n",
    "ax6_disc.plot(u1_values, grad_u1[u2_zero_idx, :], 'b-', label='∂J/∂u1', linewidth=2)\n",
    "ax6_disc.plot(u1_values, grad_u2[u2_zero_idx, :], 'r-', label='∂J/∂u2', linewidth=2)\n",
    "ax6_disc.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax6_disc.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax6_disc.set_xlabel('u1')\n",
    "ax6_disc.set_ylabel('Gradient')\n",
    "ax6_disc.set_title('Gradient Components at u2≈0')\n",
    "ax6_disc.legend()\n",
    "ax6_disc.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics about discontinuities\n",
    "print(\"Discontinuity Analysis:\")\n",
    "print(f\"Total variation range: [{total_variation.min():.4f}, {total_variation.max():.4f}]\")\n",
    "print(f\"Mean total variation: {total_variation.mean():.4f}\")\n",
    "print(f\"90th percentile threshold: {threshold:.4f}\")\n",
    "print(f\"\\nPercentage of points with high variation: {100 * np.sum(discontinuity_mask) / discontinuity_mask.size:.2f}%\")\n",
    "\n",
    "# Find regions with maximum discontinuity\n",
    "max_disc_idx = np.unravel_index(np.argmax(total_variation), total_variation.shape)\n",
    "print(f\"\\nMaximum discontinuity at:\")\n",
    "print(f\"  u1 = {U1[max_disc_idx]:.4f}\")\n",
    "print(f\"  u2 = {U2[max_disc_idx]:.4f}\")\n",
    "print(f\"  Total variation = {total_variation[max_disc_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c841df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_t(theta, T, n, epsilon):\n",
    "    \"\"\"\n",
    "    Compute Δt_k = ε + (T - nε) * exp(θ_k) / sum_j exp(θ_j)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        Array of θ_k values.\n",
    "    T : float\n",
    "        Total time or scaling parameter.\n",
    "    n : int\n",
    "        Number of terms (e.g. len(theta)).\n",
    "    epsilon : float\n",
    "        Minimum time step ε.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    delta_t : np.ndarray\n",
    "        Array of Δt_k values.\n",
    "    \"\"\"\n",
    "    theta = np.array(theta)\n",
    "    softmax = np.exp(theta) / np.sum(np.exp(theta))\n",
    "    delta_t = epsilon + (T - n * epsilon) * softmax\n",
    "    return delta_t\n",
    "\n",
    "# Example usage:\n",
    "theta = [0.2, 1.0, -0.5]\n",
    "T = 10.0\n",
    "epsilon = 0.01\n",
    "n = len(theta)\n",
    "\n",
    "dt = delta_t(theta, T, n, epsilon)\n",
    "print(\"Δt_k:\", dt)\n",
    "print(\"Sum of Δt_k =\", np.sum(dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6af644",
   "metadata": {},
   "source": [
    "## Class top wrap a Casadi Function into a Torch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68041c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import casadi as ca\n",
    "import numpy as np\n",
    "\n",
    "class CasadiFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, casadi_fn, *inputs):\n",
    "        \"\"\"\n",
    "        Generic forward pass for any CasADi function.\n",
    "        inputs: Sequence of PyTorch tensors matching CasADi arguments.\n",
    "        \"\"\"\n",
    "        # 1. Check dimensions and retrieve batch size\n",
    "        # We assume the first dimension is the Batch dimension [Batch, Features]\n",
    "        batch_size = inputs[0].shape[0]\n",
    "        \n",
    "        # 2. Prepare inputs for CasADi (Batch, Features) -> (Features, Batch)\n",
    "        # CasADi .map() expects column-major data\n",
    "        inputs_np = [x.detach().cpu().numpy().T for x in inputs]\n",
    "        \n",
    "        # 3. Run CasADi function\n",
    "        # .map allows us to run the function over the whole batch at once\n",
    "        result_dm = casadi_fn.map(batch_size)(*inputs_np)\n",
    "        \n",
    "        # 4. Handle output (CasADi might return a single item or a list)\n",
    "        if isinstance(result_dm, (list, tuple)):\n",
    "            results_np = [np.array(r).T for r in result_dm]\n",
    "        else:\n",
    "            results_np = [np.array(result_dm).T]\n",
    "            \n",
    "        # 5. Save context for backward (gradients)\n",
    "        ctx.casadi_fn = casadi_fn\n",
    "        ctx.save_for_backward(*inputs)\n",
    "        ctx.batch_size = batch_size\n",
    "        ctx.n_outputs = len(results_np)\n",
    "\n",
    "        # 6. Convert back to torch\n",
    "        results_torch = [torch.from_numpy(r).float().to(inputs[0].device) for r in results_np]\n",
    "        \n",
    "        # If specific function has only 1 output, unpack it\n",
    "        return results_torch[0] if len(results_torch) == 1 else tuple(results_torch)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        \"\"\"\n",
    "        Generic backward pass using CasADi AD.\n",
    "        \"\"\"\n",
    "        inputs = ctx.saved_tensors\n",
    "        casadi_fn = ctx.casadi_fn\n",
    "        batch_size = ctx.batch_size\n",
    "        \n",
    "        # 1. Prepare Inputs and Incoming Gradients (transposed)\n",
    "        inputs_np = [x.detach().cpu().numpy().T for x in inputs]\n",
    "        grad_outputs_np = [g.detach().cpu().numpy().T for g in grad_outputs]\n",
    "        \n",
    "        # 2. Compute Jacobian-Vector Product (Reverse Mode AD)\n",
    "        # We assume the user wants gradients w.r.t all inputs\n",
    "        # We must generate the reverse-mode AD function for THIS specific function structure\n",
    "        \n",
    "        # NOTE: For maximum efficiency, this graph generation should be cached in __init__, \n",
    "        # but for generic \"conversion\" we do it here safely.\n",
    "        \n",
    "        # Reconstruct symbolic inputs based on numerical shapes\n",
    "        sym_inputs = [ca.MX.sym(f'in_{i}', inp.shape[0], 1) for i, inp in enumerate(inputs_np)]\n",
    "        sym_out = casadi_fn(*sym_inputs)\n",
    "        \n",
    "        # Ensure sym_out is a list\n",
    "        if not isinstance(sym_out, (list, tuple)):\n",
    "            sym_out = [sym_out]\n",
    "\n",
    "        # Calculate gradients: sum( adj * output )\n",
    "        # CasADi requires matching the number of output gradients\n",
    "        adj_inputs = ca.jtimes(ca.vertcat(*sym_out), ca.vertcat(*sym_inputs), ca.vertcat(*grad_outputs_np), True)\n",
    "        \n",
    "        # The result of jtimes is one giant vector; we need to split it back into input shapes\n",
    "        # This split logic can be complex; a simpler approach for a quick converter \n",
    "        # is to ask CasADi for gradients one by one or create a specific function.\n",
    "        \n",
    "        # --- Optimized Reverse Mode Mapping ---\n",
    "        # We create a function: f_bwd(inputs, grad_outputs) -> grad_inputs\n",
    "        bwd_name = f\"{casadi_fn.name()}_bwd\"\n",
    "        sym_grads = [ca.MX.sym(f'g_{i}', g.shape[0], 1) for i, g in enumerate(grad_outputs_np)]\n",
    "        \n",
    "        # Reverse mode AD\n",
    "        # gradient of (outputs dot grad_outputs) w.r.t inputs\n",
    "        out_dot_grad = 0\n",
    "        for out_node, grad_node in zip(sym_out, sym_grads):\n",
    "            out_dot_grad += ca.dot(out_node, grad_node)\n",
    "            \n",
    "        grads_per_input = ca.gradient(out_dot_grad, ca.vertcat(*sym_inputs))\n",
    "        \n",
    "        # Create the backward function\n",
    "        # Inputs: [*inputs, *grad_outputs] -> Output: [grads_per_input]\n",
    "        # Note: grads_per_input is a flat vector, might need splitting if inputs were distinct\n",
    "        # For simplicity in this generic converter, we assume inputs are vector-like enough to be vertcat'd\n",
    "        \n",
    "        bwd_fn = ca.Function(bwd_name, [*sym_inputs, *sym_grads], [grads_per_input])\n",
    "        \n",
    "        # Execute\n",
    "        all_grads_dm = bwd_fn.map(batch_size)(*inputs_np, *grad_outputs_np)\n",
    "        all_grads_np = np.array(all_grads_dm) # Shape: (Total_Input_Dim, Batch)\n",
    "        \n",
    "        # Split gradients back to match input tensors\n",
    "        grad_inputs_torch = []\n",
    "        idx = 0\n",
    "        for inp in inputs_np:\n",
    "            rows = inp.shape[0]\n",
    "            grad_segment = all_grads_np[idx : idx+rows, :]\n",
    "            grad_inputs_torch.append(torch.from_numpy(grad_segment.T).float().to(inputs[0].device))\n",
    "            idx += rows\n",
    "            \n",
    "        return (None, *grad_inputs_torch)\n",
    "\n",
    "class CasadiToTorch(nn.Module):\n",
    "    def __init__(self, casadi_fn):\n",
    "        super().__init__()\n",
    "        self.casadi_fn = casadi_fn\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        # The apply method handles the autograd linking\n",
    "        return CasadiFunction.apply(self.casadi_fn, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5221f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes: u1: torch.Size([32, 20]), u2: torch.Size([32, 10])\n",
      "Output shape: torch.Size([32, 30])\n",
      "Gradient of first input: tensor([[-1.3953e+05, -4.7544e+05, -9.8642e+04,  3.9861e+04,  2.7634e+04,\n",
      "          3.0189e+05,  5.9914e+04,  3.9670e+04, -2.6782e+04,  1.0059e+04,\n",
      "          3.3642e+04,  1.3058e+03,  3.5546e+03,  2.1279e+02,  1.4075e+02,\n",
      "         -3.2698e+02, -1.9437e+02,  2.9531e+02,  3.6018e+00, -1.1251e+00],\n",
      "        [ 4.1306e+07,  2.2889e+08,  4.3963e+07, -4.8989e+07,  2.1738e+04,\n",
      "         -3.8379e+05, -9.5960e+04, -4.9556e+05,  3.0802e+05, -2.1140e+05,\n",
      "          2.0779e+04,  1.3790e+04, -8.9711e+03, -4.9281e+03,  9.5540e+03,\n",
      "          1.1771e+04, -6.8310e+03, -4.5519e+03, -3.4519e+03, -6.0129e+03],\n",
      "        [-2.1482e+00, -3.0445e-01, -2.5798e+00,  2.6763e-01,  1.2575e+00,\n",
      "         -8.9922e+00,  6.9783e-01, -1.7331e-01, -4.3527e-01, -1.9193e+00,\n",
      "          4.0935e-02, -4.5655e-02,  7.9704e-02, -2.2496e-02,  1.3103e-02,\n",
      "          1.6451e-02, -9.2621e-02, -6.4626e-02,  1.0451e-02,  7.7508e-03],\n",
      "        [ 8.9937e+10, -4.2581e+11,  6.3672e+11,  8.9306e+11, -2.9769e+11,\n",
      "          9.1153e+10, -1.4084e+09,  1.2005e+09,  8.0154e+07, -4.1988e+07,\n",
      "         -1.5021e+08,  8.3644e+07,  6.0465e+04, -3.3557e+04, -4.7027e+06,\n",
      "          2.1617e+06,  9.5099e+04, -1.9084e+03, -1.4584e+05,  4.7644e+04],\n",
      "        [ 1.9791e+17, -1.8817e+17,  1.1896e+15, -4.5984e+15, -4.6244e+12,\n",
      "          1.4508e+12, -5.6814e+13,  2.6336e+13,  8.0492e+13, -6.2685e+13,\n",
      "          4.5756e+11, -4.7761e+11,  5.8064e+10, -5.5002e+10, -1.2974e+09,\n",
      "         -2.7466e+08, -2.0514e+06,  1.3853e+05, -5.1715e+06, -4.6531e+05],\n",
      "        [-2.0880e+06,  8.5053e+06, -1.3103e+06,  3.1200e+06,  2.3208e+04,\n",
      "         -7.3165e+04,  4.7169e+04, -7.2968e+04, -2.2166e+05,  2.9669e+05,\n",
      "         -4.3189e+04,  8.7199e+04, -3.9723e+03,  2.8093e+03, -2.7666e+02,\n",
      "          5.0944e+02, -3.8985e+02,  3.6248e+02, -1.8645e+02,  4.8991e+01],\n",
      "        [-1.0304e+07, -1.4239e+09,  2.0217e+08,  1.2638e+09, -3.8328e+08,\n",
      "         -3.0738e+09, -7.4166e+08,  1.9695e+09,  1.6226e+07, -5.6717e+06,\n",
      "         -8.6291e+07,  1.4002e+07,  2.6341e+07, -3.2786e+06, -7.8811e+06,\n",
      "         -3.7404e+06, -1.4099e+07,  7.9689e+06, -1.1011e+04,  4.2752e+03],\n",
      "        [ 1.8705e+08,  1.5968e+09,  6.7592e+05, -3.6092e+06, -1.0372e+09,\n",
      "         -4.1103e+09,  1.4368e+09,  2.0588e+09, -9.1235e+05,  7.8366e+07,\n",
      "          3.8511e+07,  7.3983e+07, -5.5341e+05, -1.4447e+06, -2.4572e+06,\n",
      "         -8.2287e+06, -1.8330e+06,  5.0556e+06, -7.4203e+05,  1.3818e+06],\n",
      "        [ 1.8030e+08, -4.4785e+08,  4.7914e+06, -3.7546e+07,  1.8615e+08,\n",
      "          1.4085e+09,  1.6246e+08, -8.0450e+08,  2.0651e+06, -2.5096e+06,\n",
      "          8.4882e+04, -1.4874e+04, -5.0396e+03,  4.5385e+01,  2.9356e+03,\n",
      "          7.6067e+02, -1.1584e+04, -2.9390e+03, -2.4631e+03, -1.9936e+03],\n",
      "        [ 8.1770e+16, -5.4411e+16,  4.2771e+14, -8.8537e+14,  1.5368e+14,\n",
      "         -8.8947e+13,  3.4903e+11, -1.9267e+12,  2.3540e+11, -2.1239e+11,\n",
      "          2.6999e+11, -1.2019e+11,  3.4554e+09, -2.7247e+09,  3.8639e+06,\n",
      "         -2.5144e+07,  4.9955e+07,  9.5374e+06, -7.5098e+07,  6.4907e+07],\n",
      "        [-1.8167e+06,  1.1221e+06, -3.6145e+03,  1.7636e+02, -5.0230e+02,\n",
      "          5.1935e+03,  5.8942e+02,  6.7736e+02,  6.6820e-01, -1.3491e+03,\n",
      "         -6.6015e+01, -1.3131e+04, -5.7430e+02,  1.9236e+04, -4.8229e+03,\n",
      "         -2.0129e+04, -1.6568e+04, -8.6114e+04, -2.5142e+04, -4.5044e+04],\n",
      "        [-1.1069e+17,  1.0290e+17, -1.1969e+15, -2.1338e+15, -4.5732e+14,\n",
      "         -9.4521e+14,  3.1429e+14,  3.7735e+14,  4.0591e+12, -8.1921e+12,\n",
      "          2.2789e+13,  4.6202e+13,  2.4575e+12,  2.8153e+12, -3.3650e+11,\n",
      "          5.4795e+11, -5.4880e+10,  1.0428e+11, -2.0744e+09, -2.9000e+09],\n",
      "        [ 2.0297e+07,  4.9431e+06, -6.9867e+06,  5.5995e+05,  9.0210e+06,\n",
      "         -5.9632e+06, -8.7079e+05,  2.4173e+05,  3.5384e+05, -4.6242e+05,\n",
      "         -6.3822e+05,  7.3276e+05,  9.9593e+05, -6.9160e+05, -7.5083e+03,\n",
      "         -1.0703e+04, -5.4023e+02, -1.0346e+03,  8.8710e+02,  1.0656e+03],\n",
      "        [ 1.5400e+04, -3.0278e+05,  1.1695e+05,  6.1872e+05, -7.2083e+04,\n",
      "          4.5821e+04,  6.5057e+03, -5.0337e+03, -7.0722e+03,  7.3139e+03,\n",
      "         -1.2133e+04,  1.8122e+04, -2.2607e+03,  1.4699e+03, -4.0761e+02,\n",
      "          3.4052e+02, -1.9439e+02,  1.0254e+02, -3.3234e+00,  1.8710e+00],\n",
      "        [ 1.3911e+22, -1.4810e+22, -2.8142e+19, -6.3725e+20,  5.3134e+19,\n",
      "         -2.0176e+19, -3.5219e+17,  2.3124e+17,  3.2719e+18, -2.7623e+18,\n",
      "          6.5677e+15, -6.8381e+15,  1.0975e+15, -8.3953e+14,  1.0652e+13,\n",
      "         -1.8127e+12,  3.6296e+12,  4.8016e+11, -7.8190e+10, -2.2876e+08],\n",
      "        [-5.3242e-02, -6.6987e-01, -3.0150e+00, -8.7977e+00, -1.9447e+01,\n",
      "         -8.3992e+01,  1.0291e+01,  2.1022e+01, -1.8071e+01,  4.2474e+01,\n",
      "         -1.2120e-01, -4.3767e-01,  8.9714e-01,  3.5473e+00, -1.5919e+00,\n",
      "         -5.4705e+00,  1.0764e+01, -3.5360e+01, -9.2704e+00,  1.5615e+01],\n",
      "        [-3.0003e+08,  8.8506e+08, -3.1466e+08, -7.0214e+08, -4.3694e+07,\n",
      "         -1.4426e+08,  6.1592e+07,  3.5168e+07,  1.0849e+05, -9.3337e+05,\n",
      "          6.2017e+06,  1.8493e+07, -4.3234e+04,  2.2748e+05, -2.6753e+05,\n",
      "         -8.5302e+05, -1.6808e+05,  2.8727e+05, -1.7519e+05,  1.5971e+05],\n",
      "        [ 1.3762e+08,  1.5255e+07,  5.0528e+06, -2.2922e+07,  1.1834e+02,\n",
      "         -4.5996e+02, -1.9268e+05,  7.6381e+04,  1.7857e+05, -2.3954e+05,\n",
      "         -2.5669e+05,  5.3098e+05,  3.8896e+05, -2.1742e+05, -1.2604e+05,\n",
      "          1.2188e+05, -1.7034e+02,  1.5550e+02, -5.2099e+01,  3.5066e+01],\n",
      "        [-2.5946e+07, -1.0031e+07, -5.5346e+06, -1.8260e+07, -1.8351e+07,\n",
      "         -7.8226e+07,  2.5170e+06,  1.6496e+07,  3.7985e+06, -2.7522e+07,\n",
      "          5.6083e+06, -2.8559e+08, -1.2389e+08,  1.8839e+08, -7.0933e+06,\n",
      "          7.1800e+06, -3.2943e+04,  1.6176e+05, -1.5750e+04, -3.0368e+04],\n",
      "        [ 3.7927e+05,  2.1201e+06,  3.6349e+05, -8.3022e+05,  1.9300e+05,\n",
      "          7.3578e+05,  4.7139e+05, -5.0102e+05,  2.0051e+04, -1.1300e+04,\n",
      "         -2.2627e+02,  3.3327e+03,  1.2149e+03, -1.9623e+03,  2.4162e+01,\n",
      "          3.6502e+02,  6.4075e+00,  7.3308e+01,  6.1444e-01,  3.1650e+00],\n",
      "        [ 1.1407e+03, -2.2132e+03,  3.7894e-02, -2.2401e+02,  1.0920e+03,\n",
      "         -1.7014e+04, -7.8825e+03,  7.3212e+03, -2.1865e+03,  9.1385e+02,\n",
      "         -5.8498e+01, -1.5432e+01,  1.8878e+01, -3.7947e-01,  8.6902e+00,\n",
      "          1.4330e+00, -9.1492e+00, -1.9822e+00,  5.4559e+00,  4.7913e-01],\n",
      "        [ 1.0256e+05,  1.8180e+06,  3.5336e+05, -1.0616e+06,  6.6626e+03,\n",
      "         -1.4593e+04,  1.6924e+02, -1.2421e+03,  2.6820e+02, -5.7824e+02,\n",
      "         -9.2613e+01,  4.7716e+02, -2.7393e+01,  5.9499e+01,  7.4869e-01,\n",
      "          8.2889e+00, -1.7867e-01,  4.4729e-01, -1.2120e-04,  8.9809e-02],\n",
      "        [ 9.2231e+01,  2.8451e+02,  1.1118e+02,  1.5274e+02,  1.0046e+02,\n",
      "          2.1082e+02, -3.9217e+02,  3.7219e+01,  1.5149e+01,  3.0599e+02,\n",
      "         -2.4915e+02, -3.6936e+02,  2.3741e-01, -8.3765e+00, -9.4544e-01,\n",
      "          7.7836e+00, -2.1243e-03, -1.6182e-02, -4.5943e-01,  1.2316e-01],\n",
      "        [ 7.3993e+02, -4.3572e+03,  8.9176e+01, -3.9367e+03,  1.9766e+02,\n",
      "         -5.6097e+02, -2.7111e-01,  9.6666e+01, -4.0931e-01,  7.8055e+00,\n",
      "          4.6666e-01,  2.3728e+00,  4.3722e-02,  7.5757e-02,  7.4410e-02,\n",
      "          1.5772e-01,  1.8765e-01,  4.5313e-01,  6.2898e-02,  6.1776e-02],\n",
      "        [ 1.2193e+06, -1.1941e+06,  8.1528e+03,  3.2479e+04,  7.8006e+04,\n",
      "          2.9471e+05, -7.1531e+04, -1.5108e+05, -3.9231e+02, -9.0473e+02,\n",
      "          3.6875e+03,  6.0295e+03,  6.3572e+03,  4.9671e+03, -8.5099e+03,\n",
      "         -8.2568e+03, -3.1935e+03, -6.5490e+03,  7.6397e+02,  6.9537e+02],\n",
      "        [-2.5663e+03, -3.6596e+04, -5.5472e+03,  1.9252e+04, -5.3545e+02,\n",
      "         -3.0222e+02, -9.7166e+01, -3.8330e+01,  1.8207e+01,  7.2484e+00,\n",
      "         -6.4450e+01, -4.7202e+01,  3.7607e+01,  1.6943e+01, -2.9646e+01,\n",
      "         -2.2982e+01, -6.8459e+00, -6.7263e+00,  2.8280e+01,  1.8477e+01],\n",
      "        [-5.5645e+08,  1.6643e+09, -2.7218e+08,  2.0936e+07,  4.6311e+06,\n",
      "          9.1499e+06, -3.4150e+07,  4.4686e+06,  5.3957e+07, -4.2466e+08,\n",
      "          2.3953e+08,  5.2414e+08, -5.0250e+06,  1.1177e+07, -2.3003e+06,\n",
      "         -3.0530e+06,  2.3928e+06, -1.1822e+07, -2.9975e+06,  4.5528e+06],\n",
      "        [-4.0222e+10, -8.1365e+11, -1.5107e+10,  5.9869e+11, -3.5241e+10,\n",
      "          5.7886e+10, -2.7973e+09,  7.2019e+09, -5.9399e+08,  1.2040e+09,\n",
      "         -8.7626e+07,  3.8132e+08, -6.9939e+07,  1.0748e+08, -5.1226e+06,\n",
      "          3.1024e+08, -5.8064e+06,  7.5234e+07, -3.5771e+07, -5.9095e+07],\n",
      "        [ 3.1911e+06, -2.4376e+06,  3.9185e+03, -1.1816e+04, -4.8016e+03,\n",
      "          1.0444e+05,  4.0985e+04, -1.1816e+05, -4.5970e+03, -4.8353e+03,\n",
      "          5.3489e+03,  1.9396e+03, -1.1306e+03, -1.5460e+02,  1.8256e+01,\n",
      "         -3.2536e+01,  2.2896e+02, -3.0983e+01, -3.2443e+02, -4.1068e+01],\n",
      "        [ 1.2766e+12, -1.2663e+09, -1.2459e+10,  3.1736e+09,  1.3970e+10,\n",
      "         -6.6603e+09, -7.5575e+08,  2.1653e+08,  4.7030e+07, -9.6521e+07,\n",
      "         -2.2402e+08,  1.1813e+08,  1.9182e+08, -1.9811e+08,  1.8063e+08,\n",
      "         -7.3488e+08,  1.2959e+05,  3.8754e+07,  2.8471e+04,  1.3290e+05],\n",
      "        [ 1.1705e+01, -4.5316e+00, -4.9222e-02,  2.1702e-01,  1.3111e-01,\n",
      "          3.2729e-01,  1.0425e-01,  1.6443e-01,  4.0104e-01,  5.8729e-01,\n",
      "         -4.6871e-01, -3.3923e-01,  2.0183e-02,  3.4171e-02,  7.2280e-02,\n",
      "          1.2733e-01,  2.1429e-01,  2.4782e-01,  1.5072e-02, -1.7810e-02],\n",
      "        [-8.2384e+01,  5.5711e+01,  7.5564e+00, -4.6436e+00, -6.4672e+00,\n",
      "          5.1761e+01, -3.4982e+01, -8.7997e+00, -1.6492e+00, -9.0992e+01,\n",
      "          1.2404e+01,  5.3923e+01, -8.7588e+00, -5.1337e+01, -4.5669e+01,\n",
      "         -1.1089e+02, -6.8153e+01,  1.9114e+02, -3.0404e+00,  5.6550e+00]])\n"
     ]
    }
   ],
   "source": [
    "# 1. Your existing CasADi setup\n",
    "# Get the cost and gradient functions from the switched problem\n",
    "n_phases = 10\n",
    "n_inputs = 2\n",
    "J_func, grad_J_func, cost_function, gradient_function = switched_problem(n_phases=10)\n",
    "\n",
    "# 2. Convert to Torch Layer\n",
    "torch_grad_J = CasadiToTorch(grad_J_func)\n",
    "\n",
    "# 3. Use in PyTorch\n",
    "# Define batch size\n",
    "B = 32\n",
    "\n",
    "# Create dummy tensors matching your specific u and delta dimensions\n",
    "# Example: if u has 3 elements and delta has 2 elements -> 5 arguments total\n",
    "# Important: Inputs must be (Batch_Size, Dimensions)\n",
    "u1_tensor = torch.randn(B, n_inputs*n_phases, requires_grad=True)\n",
    "u2_tensor = torch.randn(B, n_phases, requires_grad=True)\n",
    "# ... etc for all inputs in [*u, *delta]\n",
    "print(f\"Input shapes: u1: {u1_tensor.shape}, u2: {u2_tensor.shape}\")\n",
    "\n",
    "\n",
    "# 4. Call it (Forward pass)\n",
    "output = torch_grad_J(u1_tensor, u2_tensor) # Add all your arguments here\n",
    "\n",
    "# 5. Use Torch methods\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "loss = output.mean()\n",
    "loss.backward() # This will differentiate through your CasADi gradient function\n",
    "\n",
    "print(f\"Gradient of first input: {u1_tensor.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
