{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3520cd24",
   "metadata": {},
   "source": [
    "# The aim of this file is to inspect the behavior of the NN's params with respect to the gradient of the cost function w.r.t. u and $\\delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c936e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, subprocess, sys\n",
    "import scipy.io\n",
    "from scipy.linalg import solve_continuous_are\n",
    "from scipy.special import softmax\n",
    "from typing import Optional, Callable, Tuple, Dict, List\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    warnings.warn(\"PyTorch not available. GPU training will not be available.\")\n",
    "    \n",
    "from ocslc.switched_linear_mpc import SwitchedLinearMPC as SwiLin_casadi\n",
    "\n",
    "from src.switched_linear_torch import SwiLin\n",
    "from src.training import SwiLinNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de771d10",
   "metadata": {},
   "source": [
    "## Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4733132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings\n",
    "N_PHASES = 10\n",
    "TIME_HORIZON = 1.0\n",
    "\n",
    "# NN settings\n",
    "N_CONTROL_INPUTS = 1\n",
    "N_STATES = 1\n",
    "N_NN_INPUTS = 1\n",
    "N_NN_OUTPUTS = N_PHASES * (N_CONTROL_INPUTS + 1)  # +1 for the mode\n",
    "\n",
    "# Casadi settings\n",
    "MULTIPLE_SHOOTING = True\n",
    "INTEGRATOR = 'exp'\n",
    "HYBRID = False\n",
    "PLOT = 'display'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd51344",
   "metadata": {},
   "source": [
    "## Compute cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6721a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cost_functional_batch(\n",
    "    swi: SwiLin,\n",
    "    u_all_batch: torch.Tensor,\n",
    "    delta_all_batch: torch.Tensor,\n",
    "    x0_batch: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Vectorized evaluation of the LQR-style cost over a batch.\n",
    "\n",
    "    Args:\n",
    "        swi: SwiLin instance (used for model matrices and helpers)\n",
    "        u_all_batch: tensor shape (B, N_PHASES, n_inputs)\n",
    "        delta_all_batch: tensor shape (B, N_PHASES)\n",
    "        x0_batch: tensor shape (B, n_states)\n",
    "\n",
    "    Returns:\n",
    "        J_batch: tensor shape (B,) with per-sample costs\n",
    "    \"\"\"\n",
    "    device = u_all_batch.device if torch.is_tensor(u_all_batch) else swi.device\n",
    "    dtype = u_all_batch.dtype if torch.is_tensor(u_all_batch) else swi.dtype\n",
    "\n",
    "    B = u_all_batch.shape[0]\n",
    "    n_ph = swi.n_phases\n",
    "    n_x = swi.n_states\n",
    "    n_u = swi.n_inputs\n",
    "\n",
    "    # Ensure tensors on correct device/dtype\n",
    "    u_all_batch = u_all_batch.to(device=device, dtype=dtype)\n",
    "    delta_all_batch = delta_all_batch.to(device=device, dtype=dtype).view(B, n_ph)\n",
    "    x0_batch = x0_batch.to(device=device, dtype=dtype).view(B, n_x)\n",
    "\n",
    "    # Containers per phase (each element will be batch-shaped)\n",
    "    Es = [None] * n_ph\n",
    "    phi_fs = [None] * n_ph\n",
    "    Lis = [None] * n_ph\n",
    "    Mis = [None] * n_ph\n",
    "    Ris = [None] * n_ph\n",
    "\n",
    "    # Useful constants\n",
    "    Q = swi.Q.to(dtype=dtype, device=device)\n",
    "    R = swi.R.to(dtype=dtype, device=device) if n_u > 0 else None\n",
    "    Eterm = swi.E_term.to(dtype=dtype, device=device)\n",
    "\n",
    "    # For each phase compute batched matrices\n",
    "    for i in range(n_ph):\n",
    "        A = swi.A[i].to(dtype=dtype, device=device)\n",
    "        Bmat = swi.B[i].to(dtype=dtype, device=device) if n_u > 0 else None\n",
    "\n",
    "        # Build big C matrix once (same across batch) as in _mat_exp_prop_exp\n",
    "        if not swi.auto:\n",
    "            m = n_u\n",
    "            Mdim = 3 * n_x + m\n",
    "            C_base = torch.zeros((Mdim, Mdim), dtype=dtype, device=device)\n",
    "            C_base[:n_x, :n_x] = -A.T\n",
    "            C_base[:n_x, n_x:2*n_x] = torch.eye(n_x, dtype=dtype, device=device)\n",
    "            C_base[n_x:2*n_x, n_x:2*n_x] = -A.T\n",
    "            C_base[n_x:2*n_x, 2*n_x:3*n_x] = Q\n",
    "            C_base[2*n_x:3*n_x, 2*n_x:3*n_x] = A\n",
    "            C_base[2*n_x:3*n_x, 3*n_x:] = Bmat\n",
    "\n",
    "            # Create batch of C scaled by delta\n",
    "            deltas_i = delta_all_batch[:, i].view(B, 1, 1)\n",
    "            deltas_i = delta_all_batch[:, i].view(B, 1, 1)\n",
    "            C_batch = C_base.unsqueeze(0) * deltas_i\n",
    "\n",
    "            # Batched matrix exponential\n",
    "            exp_C = torch.linalg.matrix_exp(C_batch)\n",
    "\n",
    "            # Extract pieces\n",
    "            F3 = exp_C[:, 2*n_x:3*n_x, 2*n_x:3*n_x]  # (B, n_x, n_x)\n",
    "            G2 = exp_C[:, n_x:2*n_x, 2*n_x:3*n_x]  # (B, n_x, n_x)\n",
    "            G3 = exp_C[:, 2*n_x:3*n_x, 3*n_x:]      # (B, n_x, m)\n",
    "            H2 = exp_C[:, n_x:2*n_x, 3*n_x:]       # (B, n_x, m)\n",
    "            K1 = exp_C[:, :n_x, 3*n_x:]            # (B, n_x, m)\n",
    "\n",
    "            Ei_batch = F3\n",
    "            Li_batch = torch.matmul(F3.transpose(-1, -2), G2)\n",
    "\n",
    "            # phi_f_i = phi_f_i_ @ ui for each sample\n",
    "            ui_batch = u_all_batch[:, i, :].view(B, n_u, 1) if n_u > 0 else None\n",
    "            if n_u > 0:\n",
    "                phi_f_i_ = G3  # (B, n_x, m)\n",
    "                # phi_f: (B, n_x, 1)\n",
    "                phi_f_batch = torch.matmul(phi_f_i_, ui_batch)\n",
    "\n",
    "                # Mi = F3.T @ H2 -> (B, n_x, m)\n",
    "                Mi_batch = torch.matmul(F3.transpose(-1, -2), H2)\n",
    "\n",
    "                # Ri: temp = B.T @ F3.T @ K1  -> (B, m, m)\n",
    "                # compute F3.T @ K1 -> (B, n_x, m)\n",
    "                tmp = torch.matmul(F3.transpose(-1, -2), K1)\n",
    "                # Bmat.T (m,n_x) @ tmp (B, n_x, m) -> (B, m, m)\n",
    "                temp = torch.matmul(Bmat.T.unsqueeze(0), tmp)\n",
    "                Ri_batch = temp + temp.transpose(-1, -2)\n",
    "            else:\n",
    "                phi_f_batch = torch.zeros((B, n_x, 1), device=device, dtype=dtype)\n",
    "                Mi_batch = torch.zeros((B, n_x, 0), device=device, dtype=dtype)\n",
    "                Ri_batch = torch.zeros((B, 0, 0), device=device, dtype=dtype)\n",
    "\n",
    "            Es[i] = Ei_batch\n",
    "            phi_fs[i] = phi_f_batch\n",
    "            Lis[i] = Li_batch\n",
    "            Mis[i] = Mi_batch\n",
    "            Ris[i] = Ri_batch\n",
    "        else:\n",
    "            # Autonomous case: simpler (Ei depends only on delta)\n",
    "            deltas_i = delta_all_batch[:, i].view(B, 1, 1)\n",
    "            Ei_batch = torch.linalg.matrix_exp(A.unsqueeze(0) * deltas_i)\n",
    "            Li_batch = torch.zeros((B, n_x, n_x), device=device, dtype=dtype)\n",
    "            Es[i] = Ei_batch\n",
    "            phi_fs[i] = torch.zeros((B, n_x, 1), device=device, dtype=dtype)\n",
    "            Lis[i] = Li_batch\n",
    "            Mis[i] = torch.zeros((B, n_x, 0), device=device, dtype=dtype)\n",
    "            Ris[i] = torch.zeros((B, 0, 0), device=device, dtype=dtype)\n",
    "\n",
    "    # Backward recursion to compute S0 per sample\n",
    "    # Initialize S_prev as (B, n_x+1, n_x+1)\n",
    "    E_aug = torch.zeros((n_x+1, n_x+1), device=device, dtype=dtype)\n",
    "    E_aug[:n_x, :n_x] = Eterm\n",
    "    S_prev = 0.5 * E_aug.unsqueeze(0).expand(B, n_x+1, n_x+1).clone()\n",
    "\n",
    "    for i in range(n_ph-1, -1, -1):\n",
    "        Ei_b = Es[i]\n",
    "        phi_f_b = phi_fs[i]\n",
    "        Li_b = Lis[i]\n",
    "        Mi_b = Mis[i]\n",
    "        Ri_b = Ris[i]\n",
    "\n",
    "        # Build S_int batch\n",
    "        S_int = torch.zeros((B, n_x+1, n_x+1), device=device, dtype=dtype)\n",
    "        S_int[:, :n_x, :n_x] = Li_b\n",
    "\n",
    "        if n_u > 0:\n",
    "            ui_col = u_all_batch[:, i, :].view(B, n_u, 1)\n",
    "            # Mi_b: (B, n_x, n_u) -> Mi_ui: (B, n_x, 1)\n",
    "            Mi_ui = torch.matmul(Mi_b, ui_col)\n",
    "            S_int[:, :n_x, n_x:] = Mi_ui\n",
    "            S_int[:, n_x:, :n_x] = Mi_ui.transpose(-1, -2)\n",
    "            # scalar term: ui^T Ri ui -> (B,1,1)\n",
    "            tmp = torch.matmul(Ri_b, ui_col)  # (B, n_u, 1)\n",
    "            uiRiui = torch.matmul(ui_col.transpose(-1, -2), tmp)  # (B,1,1)\n",
    "            S_int[:, n_x:, n_x:] = uiRiui\n",
    "\n",
    "        # Build phi batch (B, n_x+1, n_x+1)\n",
    "        phi = torch.zeros((B, n_x+1, n_x+1), device=device, dtype=dtype)\n",
    "        phi[:, :n_x, :n_x] = Ei_b\n",
    "        phi[:, :n_x, n_x:n_x+1] = phi_f_b\n",
    "        phi[:, -1, -1] = 1.0\n",
    "\n",
    "        # S_curr = 0.5*S_int + phi^T * S_prev * phi\n",
    "        S_curr = 0.5 * S_int + torch.matmul(phi.transpose(-1, -2), torch.matmul(S_prev, phi))\n",
    "        S_prev = S_curr\n",
    "\n",
    "    S0_batch = S_prev\n",
    "\n",
    "    # Augment x0 for bilinear form\n",
    "    x0_aug = torch.cat([x0_batch.view(B, n_x, 1), torch.ones((B, 1, 1), device=device, dtype=dtype)], dim=1)\n",
    "\n",
    "    # Compute quadratic term: 0.5 * x0_aug^T * S0 * x0_aug -> (B,1,1)\n",
    "    quad = torch.matmul(x0_aug.transpose(-1, -2), torch.matmul(S0_batch, x0_aug)).squeeze(-1).squeeze(-1)\n",
    "\n",
    "    # Compute G per sample\n",
    "    if n_u > 0:\n",
    "        # u_all_batch: (B, n_ph, n_u)\n",
    "        per_phase_terms = []\n",
    "        for i in range(n_ph):\n",
    "            u_b = u_all_batch[:, i, :]  # (B, n_u)\n",
    "            # (B, n_u) @ (n_u,n_u) -> (B, n_u)\n",
    "            uR = torch.matmul(u_b, R)\n",
    "            per = (uR * u_b).sum(dim=1)  # (B,)\n",
    "            per_phase_terms.append(0.5 * per * delta_all_batch[:, i])\n",
    "\n",
    "        G0 = torch.stack(per_phase_terms, dim=1).sum(dim=1)  # (B,)\n",
    "    else:\n",
    "        G0 = torch.zeros(B, device=device, dtype=dtype)\n",
    "\n",
    "    J_batch = 0.5 * quad + G0\n",
    "    return J_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e779023",
   "metadata": {},
   "source": [
    "## Compute cost gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "768aff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gradient_batch(\n",
    "    swi: SwiLin,\n",
    "    u_all_batch: torch.Tensor,\n",
    "    delta_all_batch: torch.Tensor,\n",
    "    x0_batch: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Vectorized evaluation of the gradient of the LQR-style cost over a batch.\n",
    "\n",
    "    Args:\n",
    "        swi: SwiLin instance (used for model matrices and helpers)\n",
    "        u_all_batch: tensor shape (B, N_PHASES, n_inputs)\n",
    "        delta_all_batch: tensor shape (B, N_PHASES)\n",
    "        x0_batch: tensor shape (B, n_states)\n",
    "    Returns:\n",
    "        grad_u_batch: tensor shape (B, N_PHASES, n_inputs)\n",
    "        grad_delta_batch: tensor shape (B, N_PHASES)\n",
    "    \"\"\"\n",
    "    device = u_all_batch.device if torch.is_tensor(u_all_batch) else swi.device\n",
    "    dtype = u_all_batch.dtype if torch.is_tensor(u_all_batch) else swi.dtype\n",
    "\n",
    "    B = u_all_batch.shape[0]\n",
    "    n_ph = swi.n_phases\n",
    "    n_x = swi.n_states\n",
    "    n_u = swi.n_inputs\n",
    "\n",
    "    # Ensure tensors on correct device/dtype\n",
    "    u_all_batch = u_all_batch.to(device=device, dtype=dtype)\n",
    "    delta_all_batch = delta_all_batch.to(device=device, dtype=dtype).view(B, n_ph)\n",
    "    x0_batch = x0_batch.to(device=device, dtype=dtype).view(B, n_x)\n",
    "    \n",
    "    time_steps = 256  # Number of steps for numerical integration\n",
    "    num_steps = 256   # Number of steps for inner numerical integration\n",
    "    \n",
    "    # Ensure time_steps is a tensor constant for graph compatibility\n",
    "    time_steps_t = torch.tensor(time_steps, dtype=dtype, device=device)\n",
    "    num_steps_t = torch.tensor(num_steps, dtype=dtype, device=device)\n",
    "\n",
    "    # Containers per phase (each element will be batch-shaped)\n",
    "    # Pre-allocate tensors to maintain computational graph\n",
    "    Es = []\n",
    "    phi_fs = []\n",
    "    Lis = []\n",
    "    Mis = []\n",
    "    Ris = []\n",
    "    Hi = []\n",
    "    \n",
    "    # Container for S, C, D, and N matrices\n",
    "    S = []\n",
    "    C = []\n",
    "    D = []\n",
    "    N = []\n",
    "\n",
    "    # Useful constants - extend Q to (n_x+1, n_x+1) for augmented state\n",
    "    Q_base = swi.Q.to(dtype=dtype, device=device)\n",
    "    R = swi.R.to(dtype=dtype, device=device) if n_u > 0 else None\n",
    "    Eterm = swi.E_term.to(dtype=dtype, device=device)\n",
    "    \n",
    "    # Create augmented Q matrix (n_x+1, n_x+1) with Q in top-left corner\n",
    "    Q = torch.zeros((n_x+1, n_x+1), dtype=dtype, device=device)\n",
    "    Q[:n_x, :n_x] = Q_base\n",
    "\n",
    "    # For each phase compute batched matrices\n",
    "    for i in range(n_ph):\n",
    "        A = swi.A[i].to(dtype=dtype, device=device)\n",
    "        Bmat = swi.B[i].to(dtype=dtype, device=device) if n_u > 0 else None\n",
    "\n",
    "        # Build big C matrix once (same across batch) as in _mat_exp_prop_exp\n",
    "        if not swi.auto:\n",
    "            m = n_u\n",
    "            Mdim = 3 * n_x + m\n",
    "            C_base = torch.zeros((Mdim, Mdim), dtype=dtype, device=device)\n",
    "            C_base[:n_x, :n_x] = -A.T\n",
    "            C_base[:n_x, n_x:2*n_x] = torch.eye(n_x, dtype=dtype, device=device)\n",
    "            C_base[n_x:2*n_x, n_x:2*n_x] = -A.T\n",
    "            C_base[n_x:2*n_x, 2*n_x:3*n_x] = Q_base\n",
    "            C_base[2*n_x:3*n_x, 2*n_x:3*n_x] = A\n",
    "            C_base[2*n_x:3*n_x, 3*n_x:] = Bmat\n",
    "            \n",
    "            # Create batch of C scaled by delta\n",
    "            deltas_i = delta_all_batch[:, i].view(B, 1, 1)\n",
    "            C_batch = C_base.unsqueeze(0) * deltas_i\n",
    "\n",
    "            # Batched matrix exponential\n",
    "            exp_C = torch.linalg.matrix_exp(C_batch)\n",
    "\n",
    "            # Extract pieces\n",
    "            F3 = exp_C[:, 2*n_x:3*n_x, 2*n_x:3*n_x]  # (B, n_x, n_x)\n",
    "            G2 = exp_C[:, n_x:2*n_x, 2*n_x:3*n_x]  # (B, n_x, n_x)\n",
    "            G3 = exp_C[:, 2*n_x:3*n_x, 3*n_x:]      # (B, n_x, m)\n",
    "            H2 = exp_C[:, n_x:2*n_x, 3*n_x:]       # (B, n_x, m)\n",
    "            K1 = exp_C[:, :n_x, 3*n_x:]            # (B, n_x, m)\n",
    "\n",
    "            Ei_batch = F3\n",
    "            Li_batch = torch.matmul(F3.transpose(-1, -2), G2)\n",
    "\n",
    "            # phi_f_i = phi_f_i_ @ ui for each sample\n",
    "            ui_batch = u_all_batch[:, i, :].view(B, n_u, 1) if n_u > 0 else None\n",
    "            if n_u > 0:\n",
    "                phi_f_i_ = G3  # (B, n_x, m)\n",
    "                # phi_f: (B, n_x, 1)\n",
    "                phi_f_batch = torch.matmul(phi_f_i_, ui_batch)\n",
    "\n",
    "                # Mi = F3.T @ H2 -> (B, n_x, m)\n",
    "                Mi_batch = torch.matmul(F3.transpose(-1, -2), H2)\n",
    "\n",
    "                # Ri: temp = B.T @ F3.T @ K1  -> (B, m, m)\n",
    "                # compute F3.T @ K1 -> (B, n_x, m)\n",
    "                tmp = torch.matmul(F3.transpose(-1, -2), K1)\n",
    "                # Bmat.T (m,n_x) @ tmp (B, n_x, m) -> (B, m, m)\n",
    "                temp = torch.matmul(Bmat.T.unsqueeze(0), tmp)\n",
    "                Ri_batch = temp + temp.transpose(-1, -2)\n",
    "                \n",
    "                # Create batched H matrix for this mode: shape (B, n_u, n_x+1, n_x+1)\n",
    "                Hi_batch = torch.zeros((B, n_u, n_x+1, n_x+1), dtype=dtype, device=device)\n",
    "                for k in range(n_u):\n",
    "                    # phi_f_i_ has shape (B, n_x, n_u); put its k-th column into the top-right column\n",
    "                    Hi_batch[:, k, :n_x, n_x] = phi_f_i_[:, :, k]\n",
    "                \n",
    "                # Compute the D matrix for this phase\n",
    "                D_i = torch.zeros((B, n_u, n_x+1, n_x+1), dtype=dtype, device=device)\n",
    "                \n",
    "                # Get the delta for this specific phase\n",
    "                deltas_i = delta_all_batch[:, i]  # (B,)\n",
    "                \n",
    "                # eta grid for integrating over [0, delta] - batched version\n",
    "                # Shape: (time_steps + 1, B)\n",
    "                eta_vals = torch.linspace(0, 1, steps=time_steps + 1, device=device, dtype=dtype)\n",
    "                eta_grid = eta_vals.unsqueeze(1) * deltas_i.unsqueeze(0)  # (time_steps+1, B)\n",
    "                d_eta = deltas_i / time_steps\n",
    "                \n",
    "                for ti in range(time_steps + 1):\n",
    "                    eta = eta_grid[ti]  # (B,)\n",
    "                    # phi_a_t = expm(A, eta)  - batched version\n",
    "                    # A * eta needs broadcasting: A (n,n), eta (B,) -> (B,n,n)\n",
    "                    A_scaled = A.unsqueeze(0) * eta.view(B, 1, 1)\n",
    "                    phi_a = torch.linalg.matrix_exp(A_scaled)  # (B,n,n)\n",
    "\n",
    "                    # phi_f_t = compute_integral(A, B, 0, eta) - batched\n",
    "                    # s values for each batch element\n",
    "                    s_vals = torch.linspace(0, 1, steps=num_steps + 1, device=A.device, dtype=A.dtype)\n",
    "                    s_grid = s_vals.unsqueeze(1) * eta.unsqueeze(0)  # (num_steps+1, B)\n",
    "                    ds = eta / num_steps  # (B,)\n",
    "\n",
    "                    # exp(A*(eta - s_j)) B for each s_j - fully batched\n",
    "                    # eta (B,), s_grid (num_steps+1, B) -> eta - s (num_steps+1, B)\n",
    "                    eta_minus_s = eta.unsqueeze(0) - s_grid  # (num_steps+1, B)\n",
    "                    # Need (num_steps+1, B, n, n) matrix exponentials\n",
    "                    A_diff = A.unsqueeze(0).unsqueeze(0) * eta_minus_s.view(num_steps+1, B, 1, 1)\n",
    "                    E_all = torch.linalg.matrix_exp(A_diff)  # (num_steps+1, B, n, n)\n",
    "                    # E @ Bmat for all: (num_steps+1, B, n, n) @ (n, m) -> (num_steps+1, B, n, m)\n",
    "                    vals = torch.matmul(E_all, Bmat.unsqueeze(0).unsqueeze(0))  # (num_steps+1, B, n, m)\n",
    "\n",
    "                    # trapezoid along time dimension (dim=0)\n",
    "                    trapz_weights = torch.ones(num_steps + 1, device=device, dtype=dtype)\n",
    "                    trapz_weights[0] = 0.5\n",
    "                    trapz_weights[-1] = 0.5\n",
    "                    # Weighted sum: (num_steps+1, B, n, m) * (num_steps+1, 1, 1, 1)\n",
    "                    weighted_vals = vals * trapz_weights.view(-1, 1, 1, 1)\n",
    "                    phi_f_int = ds.view(1, B, 1, 1) * weighted_vals.sum(dim=0)  # (B, n, m)\n",
    "\n",
    "                    # phi_t = transition_matrix(phi_a_t, phi_f_t@ui) - batched\n",
    "                    # phi_f_int (B,n,m), ui (B,m,1) -> phi_fu (B,n,1)\n",
    "                    phi_fu = torch.matmul(phi_f_int, ui_batch)  # (B, n, 1)\n",
    "                    # Construct Phi (B, n_x+1, n_x+1)\n",
    "                    Phi = torch.zeros((B, n_x + 1, n_x + 1), device=device, dtype=dtype)\n",
    "                    Phi[:, :n_x, :n_x] = phi_a\n",
    "                    Phi[:, :n_x, n_x] = phi_fu.squeeze(-1)\n",
    "                    Phi[:, n_x, n_x] = 1.0\n",
    "\n",
    "                    # trapezoid weight\n",
    "                    w = 0.5 if (ti == 0 or ti == time_steps) else 1.0\n",
    "\n",
    "                    # For each control channel k, form Hij and integrand - vectorized\n",
    "                    # phi_f_int has shape (B, n_x, m)\n",
    "                    for k in range(n_u):\n",
    "                        # Hij (B, n_x+1, n_x+1) with last column from kth column of phi_f_int\n",
    "                        Hij = torch.zeros((B, n_x + 1, n_x + 1), device=device, dtype=dtype)\n",
    "                        Hij[:, :n_x, n_x] = phi_f_int[:, :, k]  # (B, n_x)\n",
    "\n",
    "                        # arg = Hij^T @ Q @ Phi + Phi^T @ Q @ Hij - batched matmul\n",
    "                        # Q is (n_x+1, n_x+1), broadcast to batch\n",
    "                        Q_ext = Q.unsqueeze(0)  # (1, n_x+1, n_x+1)\n",
    "                        term1 = torch.matmul(torch.matmul(Hij.transpose(-2, -1), Q_ext), Phi)  # (B, n_x+1, n_x+1)\n",
    "                        term2 = torch.matmul(torch.matmul(Phi.transpose(-2, -1), Q_ext), Hij)  # (B, n_x+1, n_x+1)\n",
    "                        arg = term1 + term2\n",
    "\n",
    "                        integrand = 0.5 * arg  # (B, n_x+1, n_x+1)\n",
    "                        D_i[:, k] = D_i[:, k] + w * integrand\n",
    "\n",
    "                # Finish trapezoid integration over eta - batched\n",
    "                D_i = d_eta.view(B, 1, 1, 1) * D_i  # (B, n_u, n_x+1, n_x+1)\n",
    "                \n",
    "            else:\n",
    "                phi_f_batch = torch.zeros((B, n_x, 1), device=device, dtype=dtype)\n",
    "                Mi_batch = torch.zeros((B, n_x, 0), device=device, dtype=dtype)\n",
    "                Ri_batch = torch.zeros((B, 0, 0), device=device, dtype=dtype)\n",
    "                Hi_batch = torch.zeros((B, 0, n_x+1, n_x+1), dtype=dtype, device=device)\n",
    "                D_i = torch.zeros((B, 0, n_x+1, n_x+1), dtype=dtype, device=device)\n",
    "\n",
    "            Es.append(Ei_batch)\n",
    "            phi_fs.append(phi_f_batch)\n",
    "            Lis.append(Li_batch)\n",
    "            Mis.append(Mi_batch)\n",
    "            Ris.append(Ri_batch)\n",
    "            Hi.append(Hi_batch)\n",
    "            D.append(D_i)\n",
    "        else:\n",
    "            # Autonomous case: simpler (Ei depends only on delta)\n",
    "            deltas_i = delta_all_batch[:, i].view(B, 1, 1)\n",
    "            Ei_batch = torch.linalg.matrix_exp(A.unsqueeze(0) * deltas_i)\n",
    "            Li_batch = torch.zeros((B, n_x, n_x), device=device, dtype=dtype)\n",
    "            Es.append(Ei_batch)\n",
    "            phi_fs.append(torch.zeros((B, n_x, 1), device=device, dtype=dtype))\n",
    "            Lis.append(Li_batch)\n",
    "            Mis.append(torch.zeros((B, n_x, 0), device=device, dtype=dtype))\n",
    "            Ris.append(torch.zeros((B, 0, 0), device=device, dtype=dtype))\n",
    "            Hi.append(torch.zeros((B, 0, n_x+1, n_x+1), dtype=dtype, device=device))\n",
    "            D.append(torch.zeros((B, 0, n_x+1, n_x+1), dtype=dtype, device=device))\n",
    "            \n",
    "    # Backward recursion to compute S0 per sample\n",
    "    # Initialize S_prev as (B, n_x+1, n_x+1)\n",
    "    E_aug = torch.zeros((n_x+1, n_x+1), device=device, dtype=dtype)\n",
    "    E_aug[:n_x, :n_x] = Eterm\n",
    "    S_terminal = 0.5 * E_aug.unsqueeze(0).expand(B, n_x+1, n_x+1).clone()\n",
    "    \n",
    "    # Pre-allocate S list with terminal condition at the end\n",
    "    S_list = [None] * (n_ph + 1)\n",
    "    S_list[n_ph] = S_terminal\n",
    "    S_prev = S_terminal\n",
    "\n",
    "    for i in range(n_ph-1, -1, -1):\n",
    "        Ei_b = Es[i]\n",
    "        phi_f_b = phi_fs[i]\n",
    "        Li_b = Lis[i]\n",
    "        Mi_b = Mis[i]\n",
    "        Ri_b = Ris[i]\n",
    "\n",
    "        # Build S_int batch\n",
    "        S_int = torch.zeros((B, n_x+1, n_x+1), device=device, dtype=dtype)\n",
    "        S_int[:, :n_x, :n_x] = Li_b\n",
    "\n",
    "        if n_u > 0:\n",
    "            ui_col = u_all_batch[:, i, :].view(B, n_u, 1)\n",
    "            # Mi_b: (B, n_x, n_u) -> Mi_ui: (B, n_x, 1)\n",
    "            Mi_ui = torch.matmul(Mi_b, ui_col)\n",
    "            S_int[:, :n_x, n_x:] = Mi_ui\n",
    "            S_int[:, n_x:, :n_x] = Mi_ui.transpose(-1, -2)\n",
    "            # scalar term: ui^T Ri ui -> (B,1,1)\n",
    "            tmp = torch.matmul(Ri_b, ui_col)  # (B, n_u, 1)\n",
    "            uiRiui = torch.matmul(ui_col.transpose(-1, -2), tmp)  # (B,1,1)\n",
    "            S_int[:, n_x:, n_x:] = uiRiui\n",
    "\n",
    "        # Build phi batch (B, n_x+1, n_x+1)\n",
    "        phi = torch.zeros((B, n_x+1, n_x+1), device=device, dtype=dtype)\n",
    "        phi[:, :n_x, :n_x] = Ei_b\n",
    "        phi[:, :n_x, n_x:n_x+1] = phi_f_b\n",
    "        phi[:, -1, -1] = 1.0\n",
    "\n",
    "        # S_curr = 0.5*S_int + phi^T * S_prev * phi\n",
    "        S_curr = 0.5 * S_int + torch.matmul(phi.transpose(-1, -2), torch.matmul(S_prev, phi))\n",
    "        S_list[i] = S_curr\n",
    "        S_prev = S_curr\n",
    "        \n",
    "    S0_batch = S_prev\n",
    "\n",
    "    # Compute the C and N matrices\n",
    "    C_list = []\n",
    "    N_list = []\n",
    "    \n",
    "    for i in range(n_ph):\n",
    "        A = swi.A[i].to(dtype=dtype, device=device)\n",
    "        Bmat = swi.B[i].to(dtype=dtype, device=device) if n_u > 0 else None\n",
    "        # Build batched F to preserve autograd (shape: B x (n_x+1) x (n_x+1))\n",
    "        F = torch.zeros((B, n_x+1, n_x+1), dtype=dtype, device=device)\n",
    "        # Top-left block: A (broadcasted across batch)\n",
    "        F[:, :n_x, :n_x] = A.unsqueeze(0).expand(B, n_x, n_x)\n",
    "        # Top-right column: B @ u (batched)\n",
    "        if n_u > 0:\n",
    "            ui_col = u_all_batch[:, i, :].view(B, n_u, 1)\n",
    "            F[:, :n_x, n_x:n_x+1] = torch.matmul(Bmat.unsqueeze(0), ui_col)\n",
    "        \n",
    "        # Extract the S matrix of the next phase\n",
    "        S_next = S_list[i+1]\n",
    "        H_i = Hi[i]\n",
    "        \n",
    "        # C_i: batched computation\n",
    "        Q_batch = Q.unsqueeze(0)  # (1, n_x+1, n_x+1)\n",
    "        C_i = 0.5 * Q_batch + torch.matmul(F.transpose(-2, -1), S_next) + torch.matmul(S_next, F)\n",
    "        C_list.append(C_i)\n",
    "        \n",
    "        # N matrices for each control input\n",
    "        N_i_list = []\n",
    "        for j in range(n_u):\n",
    "            Hij = H_i[:, j, :, :]  # (B, n_x+1, n_x+1)\n",
    "            # Compute N matrix\n",
    "            Nij = torch.matmul(Hij.transpose(-2, -1), S_next) + torch.matmul(S_next, Hij)\n",
    "            N_i_list.append(Nij)\n",
    "        \n",
    "        if n_u > 0:\n",
    "            N_list.append(torch.stack(N_i_list, dim=1))  # (B, n_u, n_x+1, n_x+1)\n",
    "        else:\n",
    "            N_list.append(torch.zeros((B, 0, n_x+1, n_x+1), dtype=dtype, device=device))\n",
    "    \n",
    "    # Compute gradients from C and N matrices\n",
    "    grad_u_batch = torch.zeros((B, n_ph, n_u), dtype=dtype, device=device)\n",
    "    grad_delta_batch = torch.zeros((B, n_ph), dtype=dtype, device=device)\n",
    "    \n",
    "    for i in range(n_ph):\n",
    "        C_i = C_list[i]  # (B, n_x+1, n_x+1)\n",
    "        N_i = N_list[i]  # (B, n_u, n_x+1, n_x+1)\n",
    "        D_i = D[i]  # (B, n_u, n_x+1, n_x+1)\n",
    "        \n",
    "        # Build augmented state for this phase\n",
    "        # We need to track state through phases - simplified version\n",
    "        # grad_u is derived from C matrix structure\n",
    "        if n_u > 0:\n",
    "            # Extract gradients w.r.t. controls from C matrix\n",
    "            # C has contribution from control inputs in the (n_x, n_x+1) positions\n",
    "            grad_u_batch[:, i, :] = C_i[:, :n_x, n_x].sum(dim=1, keepdim=True).expand(-1, n_u)\n",
    "            \n",
    "            # grad_delta from N and D matrices\n",
    "            # Simplified: trace of N and D contributions\n",
    "            for j in range(n_u):\n",
    "                grad_delta_batch[:, i] += (N_i[:, j] * D_i[:, j]).sum(dim=(-2, -1))\n",
    "    \n",
    "    return grad_u_batch, grad_delta_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905ce6f",
   "metadata": {},
   "source": [
    "## Custom Autograd Function for Hybrid Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0d6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunctionalWithAnalyticGradient(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd function that computes the cost functional in the forward pass\n",
    "    and uses analytical gradients in the backward pass.\n",
    "    \n",
    "    This allows PyTorch to handle the gradient flow through earlier layers\n",
    "    (like the network and transformations) while using your analytical gradient\n",
    "    for the cost functional itself.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class variable to store the last computed gradients for logging\n",
    "    last_grad_u = None\n",
    "    last_grad_delta = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, swi, controls, deltas, x0):\n",
    "        \"\"\"\n",
    "        Forward pass: compute the cost functional\n",
    "        \n",
    "        Args:\n",
    "            ctx: context object to save data for backward pass\n",
    "            swi: SwiLin system\n",
    "            controls: (B, N_PHASES, n_inputs)\n",
    "            deltas: (B, N_PHASES)\n",
    "            x0: (B, n_states)\n",
    "        \"\"\"\n",
    "        # Save for backward\n",
    "        ctx.swi = swi\n",
    "        ctx.save_for_backward(controls, deltas, x0)\n",
    "        \n",
    "        # Compute cost using existing function\n",
    "        J_batch = evaluate_cost_functional_batch(swi, controls, deltas, x0)\n",
    "        return J_batch\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: use analytical gradients\n",
    "        \n",
    "        Args:\n",
    "            grad_output: gradient of output w.r.t. params (B, n_outputs, n_parameters)\n",
    "        \n",
    "        Returns:\n",
    "            Gradients w.r.t. inputs (None for swi, then controls, deltas, x0)\n",
    "        \"\"\"\n",
    "        controls, deltas, x0 = ctx.saved_tensors\n",
    "        swi = ctx.swi\n",
    "        \n",
    "        # Compute analytical gradients\n",
    "        grad_u_batch, grad_delta_batch = evaluate_gradient_batch(\n",
    "            swi, controls, deltas, x0\n",
    "        )  # (B, N_PHASES, n_inputs), (B, N_PHASES)\n",
    "        \n",
    "        # Store for logging (store detached copies to avoid memory issues)\n",
    "        CostFunctionalWithAnalyticGradient.last_grad_u = grad_u_batch.detach().clone()\n",
    "        CostFunctionalWithAnalyticGradient.last_grad_delta = grad_delta_batch.detach().clone()\n",
    "        \n",
    "        # Chain rule: multiply by grad_output from upstream\n",
    "        # grad_output has shape (B,), need to reshape for broadcasting\n",
    "        B = grad_output.shape[0]\n",
    "        grad_output = B * grad_output.view(-1, 1, 1)  # (B, 1, 1)\n",
    "        \n",
    "        grad_controls = grad_u_batch * grad_output  # (B, N_PHASES, n_inputs)\n",
    "        \n",
    "        grad_output = grad_output.squeeze(-1)  # (B, 1)\n",
    "        grad_deltas = grad_delta_batch * grad_output  # (B, N_PHASES)\n",
    "        \n",
    "        # No gradient for x0 in this formulation (it's the initial condition)\n",
    "        # If you need gradients w.r.t. x0, you can compute them analytically too\n",
    "        grad_x0 = None\n",
    "        \n",
    "        # Return gradients in same order as forward inputs\n",
    "        # (None for swi since it's not a tensor)\n",
    "        return None, grad_controls, grad_deltas, grad_x0\n",
    "\n",
    "\n",
    "# Convenient wrapper function\n",
    "def compute_cost_with_analytic_grad(swi, controls, deltas, x0):\n",
    "    \"\"\"\n",
    "    Compute cost functional using analytical gradients in backward pass.\n",
    "    \n",
    "    Args:\n",
    "        swi: SwiLin system\n",
    "        controls: (B, N_PHASES, n_inputs)\n",
    "        deltas: (B, N_PHASES)\n",
    "        x0: (B, n_states)\n",
    "    \n",
    "    Returns:\n",
    "        J_batch: (B,) cost for each sample\n",
    "    \"\"\"\n",
    "    return CostFunctionalWithAnalyticGradient.apply(swi, controls, deltas, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37d97b",
   "metadata": {},
   "source": [
    "### How Hybrid Gradients Work\n",
    "\n",
    "**Key Concept**: PyTorch's autograd system allows you to define custom backward passes using `torch.autograd.Function`.\n",
    "\n",
    "**In this implementation:**\n",
    "\n",
    "1. **Forward Pass**: Computes the cost functional normally\n",
    "   ```python\n",
    "   J_batch = evaluate_cost_functional_batch(swi, controls, deltas, x0)\n",
    "   ```\n",
    "\n",
    "2. **Backward Pass**: Uses your analytical gradient\n",
    "   ```python\n",
    "   grad_u, grad_delta = evaluate_gradient_batch(swi, controls, deltas, x0)\n",
    "   ```\n",
    "\n",
    "3. **Automatic Chain Rule**: PyTorch automatically chains your analytical gradient with the autograd gradients from:\n",
    "   - Network layers (weights, biases)\n",
    "   - Transformations (tanh clipping, softmax, etc.)\n",
    "\n",
    "**Gradient Flow:**\n",
    "```\n",
    "Input (x0) \n",
    "  → Network(x0) \n",
    "  → Tanh clipping [autograd]\n",
    "  → Softmax normalization [autograd]\n",
    "  → Cost functional [analytical gradient]\n",
    "  → Loss\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ✅ Combine analytical efficiency with automatic differentiation convenience\n",
    "- ✅ No manual chain rule implementation needed\n",
    "- ✅ Easy to switch between modes for debugging/comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e343963",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcc013b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_hybrid_gradient(\n",
    "        network: SwiLinNN,\n",
    "        X_train: torch.Tensor,\n",
    "        y_train: Optional[torch.Tensor] = None,\n",
    "        X_val: Optional[torch.Tensor] = None,\n",
    "        y_val: Optional[torch.Tensor] = None,\n",
    "        optimizer: str = 'adam',\n",
    "        learning_rate: float = 0.001,\n",
    "        weight_decay: float = 1e-4,\n",
    "        n_epochs: int = 100,\n",
    "        batch_size: int = 32,\n",
    "        device: str = 'cpu',\n",
    "        use_analytic_gradient: bool = True,  # NEW: toggle analytical vs autograd\n",
    "        # Resampling options: regenerate new random samples every N epochs\n",
    "        resample_every: Optional[int] = None,\n",
    "        resample_fn: Optional[Callable[[int], torch.Tensor]] = None,\n",
    "        resample_val: bool = False,\n",
    "        verbose: bool = True,\n",
    "        tensorboard_logdir: Optional[str] = None,\n",
    "        log_histograms: bool = False,\n",
    "        log_gradients: bool = True,  # NEW: log analytical gradients to TensorBoard\n",
    "        save_history: bool = False,\n",
    "        save_history_path: Optional[str] = None,\n",
    "        save_model: bool = False,\n",
    "        save_model_path: Optional[str] = None,\n",
    "        early_stopping: bool = False,\n",
    "        early_stopping_patience: int = 20,\n",
    "        early_stopping_min_delta: float = 1e-6,\n",
    "        early_stopping_monitor: str = 'val_loss',\n",
    "    ) -> Tuple[torch.Tensor, Dict]:\n",
    "    \"\"\"\n",
    "    Train the neural network using hybrid gradients:\n",
    "    - PyTorch autograd for network and transformations\n",
    "    - Analytical gradients for cost functional (optional)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : SwiLinNN\n",
    "        The neural network to train\n",
    "    X_train : torch.Tensor\n",
    "        Training input data\n",
    "    use_analytic_gradient : bool, optional\n",
    "        If True, use analytical gradient for cost functional.\n",
    "        If False, use PyTorch autograd for everything.\n",
    "    log_gradients : bool, optional\n",
    "        If True and use_analytic_gradient=True, log analytical gradients to TensorBoard\n",
    "    ... (other parameters same as before)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.Tensor, Dict]\n",
    "        The trained model and training history\n",
    "    \"\"\"\n",
    "    \n",
    "    network = network.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    \n",
    "    if X_val is not None:\n",
    "        X_val = X_val.to(device)\n",
    "\n",
    "    # Setup a default resampling function if requested but none provided.\n",
    "    if resample_every is not None and resample_every > 0 and resample_fn is None:\n",
    "        try:\n",
    "            x_min = -5.0\n",
    "            x_max = 5.0\n",
    "        except Exception:\n",
    "            x_min, x_max = -1.0, 1.0\n",
    "\n",
    "        def _default_resample_fn(epoch, shape=X_train.shape, dtype=X_train.dtype, device_str=device, xmin=x_min, xmax=x_max):\n",
    "            dev = device_str\n",
    "            out = torch.empty(shape, dtype=dtype, device=dev).uniform_(xmin, xmax)\n",
    "            return out\n",
    "\n",
    "        resample_fn = _default_resample_fn\n",
    "    \n",
    "    n_samples = X_train.shape[0]\n",
    "    n_inputs = network.sys.n_inputs\n",
    "    \n",
    "    # Initialize PyTorch optimizer\n",
    "    if optimizer.lower() == 'adam':\n",
    "        torch_optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer.lower() == 'sgd':\n",
    "        torch_optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer.lower() == 'rmsprop':\n",
    "        torch_optimizer = torch.optim.RMSprop(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer '{optimizer}'. Supported: 'adam', 'sgd', 'rmsprop'\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        torch_optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [] if X_val is not None else None,\n",
    "        'epochs': [],\n",
    "        'gradient_mode': 'analytic' if use_analytic_gradient else 'autograd'\n",
    "    }\n",
    "    \n",
    "    # Early stopping setup\n",
    "    if early_stopping:\n",
    "        if early_stopping_monitor == 'val_loss' and X_val is None:\n",
    "            warnings.warn(\"Early stopping monitor is 'val_loss' but no validation data provided. Switching to 'train_loss'.\")\n",
    "            early_stopping_monitor = 'train_loss'\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Early stopping enabled: monitoring '{early_stopping_monitor}' with patience={early_stopping_patience}, min_delta={early_stopping_min_delta}\")\n",
    "            print(f\"Gradient mode: {'Analytical' if use_analytic_gradient else 'PyTorch Autograd'}\")\n",
    "    \n",
    "    # Setup TensorBoard writer if requested\n",
    "    writer = SummaryWriter(log_dir=tensorboard_logdir) if tensorboard_logdir is not None else None\n",
    "\n",
    "    # Determine history save path\n",
    "    if save_history:\n",
    "        if save_history_path is None:\n",
    "            if tensorboard_logdir is not None:\n",
    "                save_history_path = os.path.join(tensorboard_logdir, 'history.json')\n",
    "            else:\n",
    "                save_history_path = os.path.join(os.getcwd(), 'training_history.json')\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        # Optionally resample training (and validation) data\n",
    "        if resample_every is not None and resample_every > 0 and epoch > 0 and (epoch % resample_every) == 0:\n",
    "            if resample_fn is None:\n",
    "                warnings.warn(\"resample_every set but resample_fn is None; skipping resampling.\")\n",
    "            else:\n",
    "                try:\n",
    "                    new_data = resample_fn(epoch)\n",
    "                    if isinstance(new_data, (list, tuple)) and len(new_data) == 2:\n",
    "                        new_X_train, new_X_val = new_data\n",
    "                    else:\n",
    "                        new_X_train, new_X_val = new_data, None\n",
    "\n",
    "                    if not torch.is_tensor(new_X_train):\n",
    "                        new_X_train = torch.as_tensor(new_X_train)\n",
    "                    X_train = new_X_train.to(device)\n",
    "                    n_samples = X_train.shape[0]\n",
    "\n",
    "                    if resample_val and new_X_val is not None:\n",
    "                        if not torch.is_tensor(new_X_val):\n",
    "                            new_X_val = torch.as_tensor(new_X_val)\n",
    "                        X_val = new_X_val.to(device)\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"Resampled training data at epoch {epoch + 1}\")\n",
    "                except Exception as e:\n",
    "                    warnings.warn(f\"Resampling failed at epoch {epoch + 1}: {e}\")\n",
    "\n",
    "        # Create random batches\n",
    "        indices = torch.randperm(n_samples, device=device)\n",
    "        \n",
    "        for start_idx in range(0, n_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            X_batch = X_train[batch_indices]\n",
    "            current_batch_size = X_batch.shape[0]\n",
    "            \n",
    "            # Zero gradients\n",
    "            torch_optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass through network\n",
    "            output, _ = network(X_batch)\n",
    "            \n",
    "            # Apply transformations: these gradients are computed by PyTorch autograd\n",
    "            T_tensor = torch.tensor(network.sys.time_horizon, device=output.device, dtype=output.dtype)\n",
    "\n",
    "            n_control_outputs = network.n_phases * n_inputs\n",
    "            controls = output[:, :n_control_outputs]\n",
    "            delta_raw = output[:, n_control_outputs:]\n",
    "            \n",
    "            # Diffeomorphism: fix last delta to zero\n",
    "            last = delta_raw[:, -1:]\n",
    "            delta_raw_traslated = delta_raw - last\n",
    "            \n",
    "            # Softmax normalization\n",
    "            delta_normalized = F.softmax(delta_raw_traslated, dim=-1)\n",
    "            deltas = delta_normalized * T_tensor\n",
    "            \n",
    "            # Tanh-based soft clipping\n",
    "            u_min = -1.0\n",
    "            u_max = 1.0\n",
    "            u_center = (u_max + u_min) / 2.0\n",
    "            u_range = (u_max - u_min) / 2.0\n",
    "            controls = u_center + u_range * torch.tanh(controls)\n",
    "            \n",
    "            # Reshape for cost computation\n",
    "            B_batch = current_batch_size\n",
    "            controls_reshaped = controls.view(B_batch, network.n_phases, n_inputs)\n",
    "            deltas_batch = deltas.view(B_batch, network.n_phases)\n",
    "            x0_batch = X_batch\n",
    "\n",
    "            # Compute loss - choose between analytical or autograd\n",
    "            if use_analytic_gradient:\n",
    "                # Use custom autograd function with analytical gradient\n",
    "                J_batch = compute_cost_with_analytic_grad(\n",
    "                    network.sys, controls_reshaped, deltas_batch, x0_batch\n",
    "                )\n",
    "            else:\n",
    "                # Use pure PyTorch autograd\n",
    "                J_batch = evaluate_cost_functional_batch(\n",
    "                    network.sys, controls_reshaped, deltas_batch, x0_batch\n",
    "                )\n",
    "            \n",
    "            loss = J_batch.mean()\n",
    "            \n",
    "            # Backward pass - PyTorch handles gradient flow automatically\n",
    "            # If use_analytic_gradient=True: analytical grad for cost, autograd for transformations\n",
    "            # If use_analytic_gradient=False: autograd for everything\n",
    "            loss.backward()\n",
    "            \n",
    "            # Compute gradient norm for logging\n",
    "            grad_norm = None\n",
    "            if writer is not None:\n",
    "                tot = torch.tensor(0.0, device=device)\n",
    "                for p in network.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        tot = tot + p.grad.detach().to(device).pow(2).sum()\n",
    "                grad_norm = torch.sqrt(tot).item()\n",
    "\n",
    "            # Optimizer step\n",
    "            torch_optimizer.step()\n",
    "\n",
    "            # Log per-batch stats to TensorBoard\n",
    "            if writer is not None:\n",
    "                global_step = epoch * max(1, n_samples // batch_size) + n_batches\n",
    "                writer.add_scalar('train/batch_loss', loss.item(), global_step)\n",
    "                if grad_norm is not None:\n",
    "                    writer.add_scalar('train/batch_grad_norm', grad_norm, global_step)\n",
    "                \n",
    "                # Log analytical gradients if available\n",
    "                if log_gradients and use_analytic_gradient:\n",
    "                    if CostFunctionalWithAnalyticGradient.last_grad_u is not None:\n",
    "                        grad_u = CostFunctionalWithAnalyticGradient.last_grad_u\n",
    "                        grad_delta = CostFunctionalWithAnalyticGradient.last_grad_delta\n",
    "                        \n",
    "                        # Log statistics of gradients wrt all controls and deltas\n",
    "                        for i in range(network.n_phases):\n",
    "                            for j in range(n_inputs):\n",
    "                                grad_u_ij = grad_u[:, i, j]\n",
    "                                writer.add_scalar(f'gradients/grad_u_phase{i}_input{j}', grad_u_ij.mean().item(), global_step)  # Mean over batch\n",
    "                            grad_delta_i = grad_delta[:, i] \n",
    "                            writer.add_scalar(f'gradients/grad_delta_phase{i}', grad_delta_i.mean().item(), global_step) # Mean over batch\n",
    "                        \n",
    "                        # Handle batch-aware pseudoinverse logging\n",
    "                        grad_u_flat = grad_u.view(B_batch, -1)\n",
    "                        grad_delta_flat = grad_delta.view(B_batch, -1)\n",
    "                        grad_output = torch.cat([grad_u_flat, grad_delta_flat], dim=-1)  # (B, M)\n",
    "\n",
    "                        # Compute pseudoinverse of the (B x M) matrix -> (M x B)\n",
    "                        grad_output_pinv = torch.linalg.pinv(grad_output)\n",
    "\n",
    "                        # Build a sensible per-sample target vector: use per-sample output gradient norms\n",
    "                        y = grad_output.norm(dim=1)  # (B,)\n",
    "\n",
    "                        # Map target into output-coefficient space: (M x B) @ (B,) -> (M,)\n",
    "                        output_coeffs = grad_output_pinv @ y\n",
    "\n",
    "                        # Log shapes for debug (optional) and histogram of coefficients\n",
    "                        if writer is not None:\n",
    "                            writer.add_histogram('gradients/output_coeffs', output_coeffs.cpu().numpy(), global_step)\n",
    "                                                \n",
    "                        # Log histograms of gradients (optional, less frequent to reduce overhead)\n",
    "                        if log_histograms and n_batches % 10 == 0:\n",
    "                            writer.add_histogram('gradients/grad_u_hist', grad_u.cpu().numpy(), global_step)\n",
    "                            writer.add_histogram('gradients/grad_delta_hist', grad_delta.cpu().numpy(), global_step)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        avg_train_loss = epoch_loss / n_batches\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['epochs'].append(epoch)\n",
    "        \n",
    "        # Validation loss\n",
    "        if X_val is not None:\n",
    "            with torch.no_grad():\n",
    "                val_output, _ = network(X_val)\n",
    "                \n",
    "                # Transform validation output\n",
    "                n_control_outputs = network.n_phases * n_inputs\n",
    "                val_controls = val_output[:, :n_control_outputs]\n",
    "                u_min = -1.0\n",
    "                u_max = 1.0\n",
    "                u_center = (u_max + u_min) / 2.0\n",
    "                u_range = (u_max - u_min) / 2.0\n",
    "                val_controls = u_center + u_range * torch.tanh(val_controls)\n",
    "                val_delta_raw = val_output[:, n_control_outputs:]\n",
    "                val_delta_raw_last = val_delta_raw[:, -1:]\n",
    "                val_delta_raw_traslated = val_delta_raw - val_delta_raw_last\n",
    "                val_delta_normalized = F.softmax(val_delta_raw_traslated, dim=-1)\n",
    "                val_deltas = val_delta_normalized * T_tensor\n",
    "                \n",
    "                # Vectorized validation loss (always use evaluate_cost_functional_batch)\n",
    "                Bv = X_val.shape[0]\n",
    "                val_controls = val_controls.view(Bv, network.n_phases, n_inputs)\n",
    "                val_deltas = val_deltas.view(Bv, network.n_phases)\n",
    "                J_val = evaluate_cost_functional_batch(network.sys, val_controls, val_deltas, X_val)\n",
    "                avg_val_loss = J_val.mean().item()\n",
    "                history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        # Write epoch-level scalars to TensorBoard\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('train/epoch_loss', avg_train_loss, epoch)\n",
    "            writer.add_scalar('train/learning_rate', torch_optimizer.param_groups[0]['lr'], epoch)\n",
    "            if X_val is not None:\n",
    "                writer.add_scalar('val/epoch_loss', avg_val_loss, epoch)\n",
    "                \n",
    "                \n",
    "            # if log_histograms:\n",
    "            #     for name, param in network.named_parameters():\n",
    "            #         writer.add_histogram(f'params/{name}', param.detach().cpu().numpy(), epoch)\n",
    "\n",
    "        # Save history to disk each epoch if requested\n",
    "        if save_history:\n",
    "            try:\n",
    "                serial = {}\n",
    "                for k, v in history.items():\n",
    "                    if v is None:\n",
    "                        serial[k] = None\n",
    "                    elif isinstance(v, list):\n",
    "                        serial[k] = [float(x) for x in v]\n",
    "                    else:\n",
    "                        serial[k] = v\n",
    "                os.makedirs(os.path.dirname(save_history_path), exist_ok=True)\n",
    "                with open(save_history_path, 'w') as fh:\n",
    "                    json.dump(serial, fh, indent=2)\n",
    "            except Exception:\n",
    "                warnings.warn(f\"Failed to save training history to {save_history_path}\")\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (epoch + 1) % max(1, n_epochs // 10) == 0:\n",
    "            if X_val is not None:\n",
    "                print(f\"Epoch {epoch + 1}/{n_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch + 1}/{n_epochs} - Train Loss: {avg_train_loss:.6f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping:\n",
    "            current_loss = avg_val_loss if early_stopping_monitor == 'val_loss' else avg_train_loss\n",
    "            \n",
    "            if current_loss < best_loss - early_stopping_min_delta:\n",
    "                best_loss = current_loss\n",
    "                best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "                best_model_state = {k: v.cpu().clone() for k, v in network.state_dict().items()}\n",
    "                if verbose and epoch > 0:\n",
    "                    print(f\"  → New best {early_stopping_monitor}: {best_loss:.6f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if verbose and patience_counter > 0 and (epoch + 1) % max(1, n_epochs // 10) == 0:\n",
    "                    print(f\"  → No improvement for {patience_counter} epoch(s)\")\n",
    "            \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                if verbose:\n",
    "                    print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "                    print(f\"Best {early_stopping_monitor}: {best_loss:.6f} at epoch {best_epoch + 1}\")\n",
    "                \n",
    "                if best_model_state is not None:\n",
    "                    network.load_state_dict(best_model_state)\n",
    "                    if verbose:\n",
    "                        print(\"Restored best model weights\")\n",
    "                \n",
    "                break\n",
    "    \n",
    "    # Get final parameters\n",
    "    params_optimized = network.get_flat_params()\n",
    "    \n",
    "    # Optionally save the trained model parameters\n",
    "    if save_model:\n",
    "        if save_model_path is None:\n",
    "            if tensorboard_logdir is not None:\n",
    "                save_model_path = os.path.join(tensorboard_logdir, 'model_state_dict.pt')\n",
    "            else:\n",
    "                save_model_path = os.path.join(os.getcwd(), 'model_state_dict.pt')\n",
    "        try:\n",
    "            network.save(save_model_path)\n",
    "            if verbose:\n",
    "                print(f\"Saved model state_dict to: {save_model_path}\")\n",
    "        except Exception:\n",
    "            warnings.warn(f\"Failed to save model to {save_model_path}\")\n",
    "\n",
    "    # Add early stopping info to history\n",
    "    if early_stopping:\n",
    "        history['early_stopping'] = {\n",
    "            'triggered': patience_counter >= early_stopping_patience,\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_loss': best_loss,\n",
    "            'monitored_metric': early_stopping_monitor,\n",
    "            'patience': early_stopping_patience,\n",
    "            'final_epoch': epoch\n",
    "        }\n",
    "\n",
    "    # Print final losses\n",
    "    if verbose:\n",
    "        print(f\"\\nFinal Training Loss: {history['train_loss'][-1]:.6f}\")\n",
    "        if X_val is not None and history['val_loss']:\n",
    "            print(f\"Final Validation Loss: {history['val_loss'][-1]:.6f}\")\n",
    "        if early_stopping and history.get('early_stopping', {}).get('triggered', False):\n",
    "            print(f\"\\nEarly stopping was triggered:\")\n",
    "            print(f\"  Best {early_stopping_monitor}: {best_loss:.6f} at epoch {best_epoch + 1}\")\n",
    "            print(f\"  Training stopped at epoch {epoch + 1}\")\n",
    "\n",
    "    return params_optimized, history\n",
    "\n",
    "\n",
    "# Keep the old function name as an alias\n",
    "train_neural_network_analytic_gradient = train_neural_network_hybrid_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9200f4cf",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bca1fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Example: Neural Network Training\n",
      "======================================================================\n",
      "Using device: cuda\n",
      "Epoch 20/200 - Train Loss: 0.528830 - Val Loss: 0.326347\n",
      "Epoch 40/200 - Train Loss: 0.645790 - Val Loss: 0.416488\n",
      "Epoch 60/200 - Train Loss: 0.744605 - Val Loss: 0.452606\n",
      "Epoch 80/200 - Train Loss: 0.765710 - Val Loss: 0.459856\n",
      "Epoch 100/200 - Train Loss: 0.766443 - Val Loss: 0.458134\n",
      "Epoch 120/200 - Train Loss: 0.768056 - Val Loss: 0.457650\n",
      "Epoch 140/200 - Train Loss: 0.773994 - Val Loss: 0.459332\n",
      "Epoch 160/200 - Train Loss: 0.778571 - Val Loss: 0.459864\n",
      "Epoch 180/200 - Train Loss: 0.785558 - Val Loss: 0.457988\n",
      "Epoch 200/200 - Train Loss: 0.790538 - Val Loss: 0.458214\n",
      "Saved model state_dict to: /home/pietro/data-driven/learning_optimization/dump/../models/analytical_grad_32_128_128_torch_20260125_190401.pt\n",
      "\n",
      "Final Training Loss: 0.790538\n",
      "Final Validation Loss: 0.458214\n",
      "\n",
      "Training complete!\n",
      "\n",
      "To view TensorBoard logs, run:\n",
      "  tensorboard --logdir=/home/pietro/data-driven/learning_optimization/dump/../logs/20260125_190401\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Example: Neural Network Training\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate synthetic data\n",
    "torch.manual_seed(42)\n",
    "n_samples_train = 1000\n",
    "n_samples_val = 10\n",
    "\n",
    "X_train = torch.empty(n_samples_train, N_NN_INPUTS).uniform_(-5.0, 5.0)\n",
    "\n",
    "\n",
    "X_val = torch.empty(n_samples_val, N_NN_INPUTS).uniform_(-5.0, 5.0)\n",
    "\n",
    "# Create network\n",
    "network = SwiLinNN(\n",
    "    layer_sizes=[N_NN_INPUTS, 32, 128, 128, N_NN_OUTPUTS],\n",
    "    n_phases=N_PHASES,\n",
    "    activation='relu',\n",
    "    output_activation='linear',\n",
    "    # nonnegative_weights=[0, 1, 2, 3],\n",
    ")\n",
    "\n",
    "# Train\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Store the path where the script is located\n",
    "# In a Jupyter notebook __file__ is not defined, fall back to the current working directory\n",
    "try:\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    script_dir = os.getcwd()\n",
    "\n",
    "date = subprocess.check_output(['date', '+%Y%m%d_%H%M%S']).decode('utf-8').strip()\n",
    "tensorboard_logdir = os.path.join(script_dir, \"..\", \"logs\", date)\n",
    "# Extract hidden layer sizes from network architecture\n",
    "hidden_layers = network.layer_sizes[1:-1]  # Exclude input and output layers\n",
    "hidden_str = \"_\".join(map(str, hidden_layers))\n",
    "model_name = f\"analytical_grad_{hidden_str}_torch_{date}.pt\"\n",
    "models_dir = os.path.join(script_dir, \"..\", \"models\", model_name)\n",
    "\n",
    "params_opt, history = train_neural_network_analytic_gradient(\n",
    "    network=network,\n",
    "    X_train=X_train,\n",
    "    # y_train=None,\n",
    "    X_val=X_val,\n",
    "    # y_val=None,\n",
    "    optimizer='adam',\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-4,\n",
    "    n_epochs=200,\n",
    "    use_analytic_gradient=True,\n",
    "    resample_every=None,\n",
    "    resample_fn=None,\n",
    "    resample_val=False,\n",
    "    early_stopping=False,\n",
    "    early_stopping_patience=30,\n",
    "    early_stopping_min_delta=1e-4,\n",
    "    batch_size=n_samples_train,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    tensorboard_logdir=tensorboard_logdir,\n",
    "    log_histograms=True,\n",
    "    log_gradients=True,  # Enable gradient logging\n",
    "    save_model=True,\n",
    "    save_model_path=models_dir\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"\\nTo view TensorBoard logs, run:\")\n",
    "print(f\"  tensorboard --logdir={tensorboard_logdir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
