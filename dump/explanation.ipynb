{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3520cd24",
   "metadata": {},
   "source": [
    "# The aim of this file is to inspect the behavior of the NN's params with respect to the gradient of the cost function w.r.t. u and $\\delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c936e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, subprocess, sys\n",
    "import scipy.io\n",
    "from scipy.linalg import solve_continuous_are\n",
    "from scipy.special import softmax\n",
    "from typing import Optional, Callable, Tuple, Dict, List\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    warnings.warn(\"PyTorch not available. GPU training will not be available.\")\n",
    "    \n",
    "from ocslc.switched_linear_mpc import SwitchedLinearMPC as SwiLin_casadi\n",
    "\n",
    "from src.switched_linear_torch import SwiLin\n",
    "from src.training import SwiLinNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de771d10",
   "metadata": {},
   "source": [
    "## Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4733132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings\n",
    "N_PHASES = 10\n",
    "TIME_HORIZON = 1.0\n",
    "\n",
    "# NN settings\n",
    "N_CONTROL_INPUTS = 1\n",
    "N_STATES = 3\n",
    "N_NN_INPUTS = 3\n",
    "N_NN_OUTPUTS = N_PHASES * (N_CONTROL_INPUTS + 1)  # +1 for the mode\n",
    "\n",
    "# Casadi settings\n",
    "MULTIPLE_SHOOTING = True\n",
    "INTEGRATOR = 'exp'\n",
    "HYBRID = False\n",
    "PLOT = 'display'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd51344",
   "metadata": {},
   "source": [
    "## Compute cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6721a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cost_functional_batch(\n",
    "    swi: SwiLin,\n",
    "    u_all_batch: torch.Tensor,\n",
    "    delta_all_batch: torch.Tensor,\n",
    "    x0_batch: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Vectorized evaluation of the LQR-style cost over a batch.\n",
    "\n",
    "    Args:\n",
    "        swi: SwiLin instance (used for model matrices and helpers)\n",
    "        u_all_batch: tensor shape (B, n_phases, n_inputs)\n",
    "        delta_all_batch: tensor shape (B, n_phases)\n",
    "        x0_batch: tensor shape (B, n_states)\n",
    "\n",
    "    Returns:\n",
    "        J_batch: tensor shape (B,) with per-sample costs\n",
    "    \"\"\"\n",
    "    device = u_all_batch.device if torch.is_tensor(u_all_batch) else swi.device\n",
    "    dtype = u_all_batch.dtype if torch.is_tensor(u_all_batch) else swi.dtype\n",
    "\n",
    "    B = u_all_batch.shape[0]\n",
    "    n_ph = swi.n_phases\n",
    "    n_x = swi.n_states\n",
    "    n_u = swi.n_inputs\n",
    "\n",
    "    # Ensure tensors on correct device/dtype\n",
    "    u_all_batch = u_all_batch.to(device=device, dtype=dtype)\n",
    "    delta_all_batch = delta_all_batch.to(device=device, dtype=dtype).view(B, n_ph)\n",
    "    x0_batch = x0_batch.to(device=device, dtype=dtype).view(B, n_x)\n",
    "\n",
    "    # Containers per phase (each element will be batch-shaped)\n",
    "    Es = [None] * n_ph\n",
    "    phi_fs = [None] * n_ph\n",
    "    Lis = [None] * n_ph\n",
    "    Mis = [None] * n_ph\n",
    "    Ris = [None] * n_ph\n",
    "\n",
    "    # Useful constants\n",
    "    Q = swi.Q.to(dtype=dtype, device=device)\n",
    "    R = swi.R.to(dtype=dtype, device=device) if n_u > 0 else None\n",
    "    Eterm = swi.E_term.to(dtype=dtype, device=device)\n",
    "\n",
    "    # For each phase compute batched matrices\n",
    "    for i in range(n_ph):\n",
    "        A = swi.A[i].to(dtype=dtype, device=device)\n",
    "        Bmat = swi.B[i].to(dtype=dtype, device=device) if n_u > 0 else None\n",
    "\n",
    "        # Build big C matrix once (same across batch) as in _mat_exp_prop_exp\n",
    "        if not swi.auto:\n",
    "            m = n_u\n",
    "            Mdim = 3 * n_x + m\n",
    "            C_base = torch.zeros((Mdim, Mdim), dtype=dtype, device=device)\n",
    "            C_base[:n_x, :n_x] = -A.T\n",
    "            C_base[:n_x, n_x:2*n_x] = torch.eye(n_x, dtype=dtype, device=device)\n",
    "            C_base[n_x:2*n_x, n_x:2*n_x] = -A.T\n",
    "            C_base[n_x:2*n_x, 2*n_x:3*n_x] = Q\n",
    "            C_base[2*n_x:3*n_x, 2*n_x:3*n_x] = A\n",
    "            C_base[2*n_x:3*n_x, 3*n_x:] = Bmat\n",
    "\n",
    "            # Create batch of C scaled by delta\n",
    "            deltas_i = delta_all_batch[:, i].view(B, 1, 1)\n",
    "            deltas_i = delta_all_batch[:, i].view(B, 1, 1)\n",
    "            C_batch = C_base.unsqueeze(0) * deltas_i\n",
    "\n",
    "            # Batched matrix exponential\n",
    "            exp_C = torch.linalg.matrix_exp(C_batch)\n",
    "\n",
    "            # Extract pieces\n",
    "            F3 = exp_C[:, 2*n_x:3*n_x, 2*n_x:3*n_x]  # (B, n_x, n_x)\n",
    "            G2 = exp_C[:, n_x:2*n_x, 2*n_x:3*n_x]  # (B, n_x, n_x)\n",
    "            G3 = exp_C[:, 2*n_x:3*n_x, 3*n_x:]      # (B, n_x, m)\n",
    "            H2 = exp_C[:, n_x:2*n_x, 3*n_x:]       # (B, n_x, m)\n",
    "            K1 = exp_C[:, :n_x, 3*n_x:]            # (B, n_x, m)\n",
    "\n",
    "            Ei_batch = F3\n",
    "            Li_batch = torch.matmul(F3.transpose(-1, -2), G2)\n",
    "\n",
    "            # phi_f_i = phi_f_i_ @ ui for each sample\n",
    "            ui_batch = u_all_batch[:, i, :].view(B, n_u, 1) if n_u > 0 else None\n",
    "            if n_u > 0:\n",
    "                phi_f_i_ = G3  # (B, n_x, m)\n",
    "                # phi_f: (B, n_x, 1)\n",
    "                phi_f_batch = torch.matmul(phi_f_i_, ui_batch)\n",
    "\n",
    "                # Mi = F3.T @ H2 -> (B, n_x, m)\n",
    "                Mi_batch = torch.matmul(F3.transpose(-1, -2), H2)\n",
    "\n",
    "                # Ri: temp = B.T @ F3.T @ K1  -> (B, m, m)\n",
    "                # compute F3.T @ K1 -> (B, n_x, m)\n",
    "                tmp = torch.matmul(F3.transpose(-1, -2), K1)\n",
    "                # Bmat.T (m,n_x) @ tmp (B, n_x, m) -> (B, m, m)\n",
    "                temp = torch.matmul(Bmat.T.unsqueeze(0), tmp)\n",
    "                Ri_batch = temp + temp.transpose(-1, -2)\n",
    "            else:\n",
    "                phi_f_batch = torch.zeros((B, n_x, 1), device=device, dtype=dtype)\n",
    "                Mi_batch = torch.zeros((B, n_x, 0), device=device, dtype=dtype)\n",
    "                Ri_batch = torch.zeros((B, 0, 0), device=device, dtype=dtype)\n",
    "\n",
    "            Es[i] = Ei_batch\n",
    "            phi_fs[i] = phi_f_batch\n",
    "            Lis[i] = Li_batch\n",
    "            Mis[i] = Mi_batch\n",
    "            Ris[i] = Ri_batch\n",
    "        else:\n",
    "            # Autonomous case: simpler (Ei depends only on delta)\n",
    "            deltas_i = delta_all_batch[:, i].view(B, 1, 1)\n",
    "            Ei_batch = torch.linalg.matrix_exp(A.unsqueeze(0) * deltas_i)\n",
    "            Li_batch = torch.zeros((B, n_x, n_x), device=device, dtype=dtype)\n",
    "            Es[i] = Ei_batch\n",
    "            phi_fs[i] = torch.zeros((B, n_x, 1), device=device, dtype=dtype)\n",
    "            Lis[i] = Li_batch\n",
    "            Mis[i] = torch.zeros((B, n_x, 0), device=device, dtype=dtype)\n",
    "            Ris[i] = torch.zeros((B, 0, 0), device=device, dtype=dtype)\n",
    "\n",
    "    # Backward recursion to compute S0 per sample\n",
    "    # Initialize S_prev as (B, n_x+1, n_x+1)\n",
    "    E_aug = torch.zeros((n_x+1, n_x+1), device=device, dtype=dtype)\n",
    "    E_aug[:n_x, :n_x] = Eterm\n",
    "    S_prev = 0.5 * E_aug.unsqueeze(0).expand(B, n_x+1, n_x+1).clone()\n",
    "\n",
    "    for i in range(n_ph-1, -1, -1):\n",
    "        Ei_b = Es[i]\n",
    "        phi_f_b = phi_fs[i]\n",
    "        Li_b = Lis[i]\n",
    "        Mi_b = Mis[i]\n",
    "        Ri_b = Ris[i]\n",
    "\n",
    "        # Build S_int batch\n",
    "        S_int = torch.zeros((B, n_x+1, n_x+1), device=device, dtype=dtype)\n",
    "        S_int[:, :n_x, :n_x] = Li_b\n",
    "\n",
    "        if n_u > 0:\n",
    "            ui_col = u_all_batch[:, i, :].view(B, n_u, 1)\n",
    "            # Mi_b: (B, n_x, n_u) -> Mi_ui: (B, n_x, 1)\n",
    "            Mi_ui = torch.matmul(Mi_b, ui_col)\n",
    "            S_int[:, :n_x, n_x:] = Mi_ui\n",
    "            S_int[:, n_x:, :n_x] = Mi_ui.transpose(-1, -2)\n",
    "            # scalar term: ui^T Ri ui -> (B,1,1)\n",
    "            tmp = torch.matmul(Ri_b, ui_col)  # (B, n_u, 1)\n",
    "            uiRiui = torch.matmul(ui_col.transpose(-1, -2), tmp)  # (B,1,1)\n",
    "            S_int[:, n_x:, n_x:] = uiRiui\n",
    "\n",
    "        # Build phi batch (B, n_x+1, n_x+1)\n",
    "        phi = torch.zeros((B, n_x+1, n_x+1), device=device, dtype=dtype)\n",
    "        phi[:, :n_x, :n_x] = Ei_b\n",
    "        phi[:, :n_x, n_x:n_x+1] = phi_f_b\n",
    "        phi[:, -1, -1] = 1.0\n",
    "\n",
    "        # S_curr = 0.5*S_int + phi^T * S_prev * phi\n",
    "        S_curr = 0.5 * S_int + torch.matmul(phi.transpose(-1, -2), torch.matmul(S_prev, phi))\n",
    "        S_prev = S_curr\n",
    "\n",
    "    S0_batch = S_prev\n",
    "\n",
    "    # Augment x0 for bilinear form\n",
    "    x0_aug = torch.cat([x0_batch.view(B, n_x, 1), torch.ones((B, 1, 1), device=device, dtype=dtype)], dim=1)\n",
    "\n",
    "    # Compute quadratic term: 0.5 * x0_aug^T * S0 * x0_aug -> (B,1,1)\n",
    "    quad = torch.matmul(x0_aug.transpose(-1, -2), torch.matmul(S0_batch, x0_aug)).squeeze(-1).squeeze(-1)\n",
    "\n",
    "    # Compute G per sample\n",
    "    if n_u > 0:\n",
    "        # u_all_batch: (B, n_ph, n_u)\n",
    "        per_phase_terms = []\n",
    "        for i in range(n_ph):\n",
    "            u_b = u_all_batch[:, i, :]  # (B, n_u)\n",
    "            # (B, n_u) @ (n_u,n_u) -> (B, n_u)\n",
    "            uR = torch.matmul(u_b, R)\n",
    "            per = (uR * u_b).sum(dim=1)  # (B,)\n",
    "            per_phase_terms.append(0.5 * per * delta_all_batch[:, i])\n",
    "\n",
    "        G0 = torch.stack(per_phase_terms, dim=1).sum(dim=1)  # (B,)\n",
    "    else:\n",
    "        G0 = torch.zeros(B, device=device, dtype=dtype)\n",
    "\n",
    "    J_batch = 0.5 * quad + G0\n",
    "    return J_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e779023",
   "metadata": {},
   "source": [
    "## Compute cost gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "768aff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gradient_batch(\n",
    "    swi: SwiLin,\n",
    "    u_all_batch: torch.Tensor,\n",
    "    delta_all_batch: torch.Tensor,\n",
    "    x0_batch: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Vectorized evaluation of the gradient of the LQR-style cost over a batch.\n",
    "\n",
    "    Args:\n",
    "        swi: SwiLin instance (used for model matrices and helpers)\n",
    "        u_all_batch: tensor shape (B, n_phases, n_inputs)\n",
    "        delta_all_batch: tensor shape (B, n_phases)\n",
    "        x0_batch: tensor shape (B, n_states)\n",
    "    Returns:\n",
    "        grad_u_batch: tensor shape (B, n_phases, n_inputs)\n",
    "        grad_delta_batch: tensor shape (B, n_phases)\n",
    "    \"\"\"\n",
    "    device = u_all_batch.device if torch.is_tensor(u_all_batch) else swi.device\n",
    "    dtype = u_all_batch.dtype if torch.is_tensor(u_all_batch) else swi.dtype\n",
    "\n",
    "    B = u_all_batch.shape[0]\n",
    "    n_ph = swi.n_phases\n",
    "    n_x = swi.n_states\n",
    "    n_u = swi.n_inputs\n",
    "\n",
    "    # Ensure tensors on correct device/dtype\n",
    "    u_all_batch = u_all_batch.to(device=device, dtype=dtype)\n",
    "    delta_all_batch = delta_all_batch.to(device=device, dtype=dtype).view(B, n_ph)\n",
    "    x0_batch = x0_batch.to(device=device, dtype=dtype).view(B, n_x)\n",
    "    \n",
    "    time_steps = 256  # Number of steps for numerical integration\n",
    "    num_steps = 256   # Number of steps for inner numerical integration\n",
    "    \n",
    "    # Ensure time_steps is a tensor constant for graph compatibility\n",
    "    time_steps_t = torch.tensor(time_steps, dtype=dtype, device=device)\n",
    "    num_steps_t = torch.tensor(num_steps, dtype=dtype, device=device)\n",
    "\n",
    "    # Containers per phase (each element will be batch-shaped)\n",
    "    # Pre-allocate tensors to maintain computational graph\n",
    "    Es = []\n",
    "    phi_fs = []\n",
    "    Lis = []\n",
    "    Mis = []\n",
    "    Ris = []\n",
    "    Hi = []\n",
    "    \n",
    "    # Container for S, C, D, and N matrices\n",
    "    S = []\n",
    "    C = []\n",
    "    D = []\n",
    "    N = []\n",
    "\n",
    "    # Useful constants - extend Q to (n_x+1, n_x+1) for augmented state\n",
    "    Q_base = swi.Q.to(dtype=dtype, device=device)\n",
    "    R = swi.R.to(dtype=dtype, device=device) if n_u > 0 else None\n",
    "    Eterm = swi.E_term.to(dtype=dtype, device=device)\n",
    "    \n",
    "    # Create augmented Q matrix (n_x+1, n_x+1) with Q in top-left corner\n",
    "    Q = torch.zeros((n_x+1, n_x+1), dtype=dtype, device=device)\n",
    "    Q[:n_x, :n_x] = Q_base\n",
    "\n",
    "    # For each phase compute batched matrices\n",
    "    for i in range(n_ph):\n",
    "        A = swi.A[i].to(dtype=dtype, device=device)\n",
    "        Bmat = swi.B[i].to(dtype=dtype, device=device) if n_u > 0 else None\n",
    "\n",
    "        # Build big C matrix once (same across batch) as in _mat_exp_prop_exp\n",
    "        if not swi.auto:\n",
    "            m = n_u\n",
    "            Mdim = 3 * n_x + m\n",
    "            C_base = torch.zeros((Mdim, Mdim), dtype=dtype, device=device)\n",
    "            C_base[:n_x, :n_x] = -A.T\n",
    "            C_base[:n_x, n_x:2*n_x] = torch.eye(n_x, dtype=dtype, device=device)\n",
    "            C_base[n_x:2*n_x, n_x:2*n_x] = -A.T\n",
    "            C_base[n_x:2*n_x, 2*n_x:3*n_x] = Q_base\n",
    "            C_base[2*n_x:3*n_x, 2*n_x:3*n_x] = A\n",
    "            C_base[2*n_x:3*n_x, 3*n_x:] = Bmat\n",
    "            \n",
    "            # Create batch of C scaled by delta\n",
    "            deltas_i = delta_all_batch[:, i].view(B, 1, 1)\n",
    "            C_batch = C_base.unsqueeze(0) * deltas_i\n",
    "\n",
    "            # Batched matrix exponential\n",
    "            exp_C = torch.linalg.matrix_exp(C_batch)\n",
    "\n",
    "            # Extract pieces\n",
    "            F3 = exp_C[:, 2*n_x:3*n_x, 2*n_x:3*n_x]  # (B, n_x, n_x)\n",
    "            G2 = exp_C[:, n_x:2*n_x, 2*n_x:3*n_x]  # (B, n_x, n_x)\n",
    "            G3 = exp_C[:, 2*n_x:3*n_x, 3*n_x:]      # (B, n_x, m)\n",
    "            H2 = exp_C[:, n_x:2*n_x, 3*n_x:]       # (B, n_x, m)\n",
    "            K1 = exp_C[:, :n_x, 3*n_x:]            # (B, n_x, m)\n",
    "\n",
    "            Ei_batch = F3\n",
    "            Li_batch = torch.matmul(F3.transpose(-1, -2), G2)\n",
    "\n",
    "            # phi_f_i = phi_f_i_ @ ui for each sample\n",
    "            ui_batch = u_all_batch[:, i, :].view(B, n_u, 1) if n_u > 0 else None\n",
    "            if n_u > 0:\n",
    "                phi_f_i_ = G3  # (B, n_x, m)\n",
    "                # phi_f: (B, n_x, 1)\n",
    "                phi_f_batch = torch.matmul(phi_f_i_, ui_batch)\n",
    "\n",
    "                # Mi = F3.T @ H2 -> (B, n_x, m)\n",
    "                Mi_batch = torch.matmul(F3.transpose(-1, -2), H2)\n",
    "\n",
    "                # Ri: temp = B.T @ F3.T @ K1  -> (B, m, m)\n",
    "                # compute F3.T @ K1 -> (B, n_x, m)\n",
    "                tmp = torch.matmul(F3.transpose(-1, -2), K1)\n",
    "                # Bmat.T (m,n_x) @ tmp (B, n_x, m) -> (B, m, m)\n",
    "                temp = torch.matmul(Bmat.T.unsqueeze(0), tmp)\n",
    "                Ri_batch = temp + temp.transpose(-1, -2)\n",
    "                \n",
    "                # Create batched H matrix for this mode: shape (B, n_u, n_x+1, n_x+1)\n",
    "                Hi_batch = torch.zeros((B, n_u, n_x+1, n_x+1), dtype=dtype, device=device)\n",
    "                for k in range(n_u):\n",
    "                    # phi_f_i_ has shape (B, n_x, n_u); put its k-th column into the top-right column\n",
    "                    Hi_batch[:, k, :n_x, n_x] = phi_f_i_[:, :, k]\n",
    "                \n",
    "                # Compute the D matrix for this phase\n",
    "                D_i = torch.zeros((B, n_u, n_x+1, n_x+1), dtype=dtype, device=device)\n",
    "                \n",
    "                # Get the delta for this specific phase\n",
    "                deltas_i = delta_all_batch[:, i]  # (B,)\n",
    "                \n",
    "                # eta grid for integrating over [0, delta] - batched version\n",
    "                # Shape: (time_steps + 1, B)\n",
    "                eta_vals = torch.linspace(0, 1, steps=time_steps + 1, device=device, dtype=dtype)\n",
    "                eta_grid = eta_vals.unsqueeze(1) * deltas_i.unsqueeze(0)  # (time_steps+1, B)\n",
    "                d_eta = deltas_i / time_steps\n",
    "                \n",
    "                for ti in range(time_steps + 1):\n",
    "                    eta = eta_grid[ti]  # (B,)\n",
    "                    # phi_a_t = expm(A, eta)  - batched version\n",
    "                    # A * eta needs broadcasting: A (n,n), eta (B,) -> (B,n,n)\n",
    "                    A_scaled = A.unsqueeze(0) * eta.view(B, 1, 1)\n",
    "                    phi_a = torch.linalg.matrix_exp(A_scaled)  # (B,n,n)\n",
    "\n",
    "                    # phi_f_t = compute_integral(A, B, 0, eta) - batched\n",
    "                    # s values for each batch element\n",
    "                    s_vals = torch.linspace(0, 1, steps=num_steps + 1, device=A.device, dtype=A.dtype)\n",
    "                    s_grid = s_vals.unsqueeze(1) * eta.unsqueeze(0)  # (num_steps+1, B)\n",
    "                    ds = eta / num_steps  # (B,)\n",
    "\n",
    "                    # exp(A*(eta - s_j)) B for each s_j - fully batched\n",
    "                    # eta (B,), s_grid (num_steps+1, B) -> eta - s (num_steps+1, B)\n",
    "                    eta_minus_s = eta.unsqueeze(0) - s_grid  # (num_steps+1, B)\n",
    "                    # Need (num_steps+1, B, n, n) matrix exponentials\n",
    "                    A_diff = A.unsqueeze(0).unsqueeze(0) * eta_minus_s.view(num_steps+1, B, 1, 1)\n",
    "                    E_all = torch.linalg.matrix_exp(A_diff)  # (num_steps+1, B, n, n)\n",
    "                    # E @ Bmat for all: (num_steps+1, B, n, n) @ (n, m) -> (num_steps+1, B, n, m)\n",
    "                    vals = torch.matmul(E_all, Bmat.unsqueeze(0).unsqueeze(0))  # (num_steps+1, B, n, m)\n",
    "\n",
    "                    # trapezoid along time dimension (dim=0)\n",
    "                    trapz_weights = torch.ones(num_steps + 1, device=device, dtype=dtype)\n",
    "                    trapz_weights[0] = 0.5\n",
    "                    trapz_weights[-1] = 0.5\n",
    "                    # Weighted sum: (num_steps+1, B, n, m) * (num_steps+1, 1, 1, 1)\n",
    "                    weighted_vals = vals * trapz_weights.view(-1, 1, 1, 1)\n",
    "                    phi_f_int = ds.view(1, B, 1, 1) * weighted_vals.sum(dim=0)  # (B, n, m)\n",
    "\n",
    "                    # phi_t = transition_matrix(phi_a_t, phi_f_t@ui) - batched\n",
    "                    # phi_f_int (B,n,m), ui (B,m,1) -> phi_fu (B,n,1)\n",
    "                    phi_fu = torch.matmul(phi_f_int, ui_batch)  # (B, n, 1)\n",
    "                    # Construct Phi (B, n_x+1, n_x+1)\n",
    "                    Phi = torch.zeros((B, n_x + 1, n_x + 1), device=device, dtype=dtype)\n",
    "                    Phi[:, :n_x, :n_x] = phi_a\n",
    "                    Phi[:, :n_x, n_x] = phi_fu.squeeze(-1)\n",
    "                    Phi[:, n_x, n_x] = 1.0\n",
    "\n",
    "                    # trapezoid weight\n",
    "                    w = 0.5 if (ti == 0 or ti == time_steps) else 1.0\n",
    "\n",
    "                    # For each control channel k, form Hij and integrand - vectorized\n",
    "                    # phi_f_int has shape (B, n_x, m)\n",
    "                    for k in range(n_u):\n",
    "                        # Hij (B, n_x+1, n_x+1) with last column from kth column of phi_f_int\n",
    "                        Hij = torch.zeros((B, n_x + 1, n_x + 1), device=device, dtype=dtype)\n",
    "                        Hij[:, :n_x, n_x] = phi_f_int[:, :, k]  # (B, n_x)\n",
    "\n",
    "                        # arg = Hij^T @ Q @ Phi + Phi^T @ Q @ Hij - batched matmul\n",
    "                        # Q is (n_x+1, n_x+1), broadcast to batch\n",
    "                        Q_ext = Q.unsqueeze(0)  # (1, n_x+1, n_x+1)\n",
    "                        term1 = torch.matmul(torch.matmul(Hij.transpose(-2, -1), Q_ext), Phi)  # (B, n_x+1, n_x+1)\n",
    "                        term2 = torch.matmul(torch.matmul(Phi.transpose(-2, -1), Q_ext), Hij)  # (B, n_x+1, n_x+1)\n",
    "                        arg = term1 + term2\n",
    "\n",
    "                        integrand = 0.5 * arg  # (B, n_x+1, n_x+1)\n",
    "                        D_i[:, k] = D_i[:, k] + w * integrand\n",
    "\n",
    "                # Finish trapezoid integration over eta - batched\n",
    "                D_i = d_eta.view(B, 1, 1, 1) * D_i  # (B, n_u, n_x+1, n_x+1)\n",
    "                \n",
    "            else:\n",
    "                phi_f_batch = torch.zeros((B, n_x, 1), device=device, dtype=dtype)\n",
    "                Mi_batch = torch.zeros((B, n_x, 0), device=device, dtype=dtype)\n",
    "                Ri_batch = torch.zeros((B, 0, 0), device=device, dtype=dtype)\n",
    "                Hi_batch = torch.zeros((B, 0, n_x+1, n_x+1), dtype=dtype, device=device)\n",
    "                D_i = torch.zeros((B, 0, n_x+1, n_x+1), dtype=dtype, device=device)\n",
    "\n",
    "            Es.append(Ei_batch)\n",
    "            phi_fs.append(phi_f_batch)\n",
    "            Lis.append(Li_batch)\n",
    "            Mis.append(Mi_batch)\n",
    "            Ris.append(Ri_batch)\n",
    "            Hi.append(Hi_batch)\n",
    "            D.append(D_i)\n",
    "        else:\n",
    "            # Autonomous case: simpler (Ei depends only on delta)\n",
    "            deltas_i = delta_all_batch[:, i].view(B, 1, 1)\n",
    "            Ei_batch = torch.linalg.matrix_exp(A.unsqueeze(0) * deltas_i)\n",
    "            Li_batch = torch.zeros((B, n_x, n_x), device=device, dtype=dtype)\n",
    "            Es.append(Ei_batch)\n",
    "            phi_fs.append(torch.zeros((B, n_x, 1), device=device, dtype=dtype))\n",
    "            Lis.append(Li_batch)\n",
    "            Mis.append(torch.zeros((B, n_x, 0), device=device, dtype=dtype))\n",
    "            Ris.append(torch.zeros((B, 0, 0), device=device, dtype=dtype))\n",
    "            Hi.append(torch.zeros((B, 0, n_x+1, n_x+1), dtype=dtype, device=device))\n",
    "            D.append(torch.zeros((B, 0, n_x+1, n_x+1), dtype=dtype, device=device))\n",
    "            \n",
    "    # Backward recursion to compute S0 per sample\n",
    "    # Initialize S_prev as (B, n_x+1, n_x+1)\n",
    "    E_aug = torch.zeros((n_x+1, n_x+1), device=device, dtype=dtype)\n",
    "    E_aug[:n_x, :n_x] = Eterm\n",
    "    S_terminal = 0.5 * E_aug.unsqueeze(0).expand(B, n_x+1, n_x+1).clone()\n",
    "    \n",
    "    # Pre-allocate S list with terminal condition at the end\n",
    "    S_list = [None] * (n_ph + 1)\n",
    "    S_list[n_ph] = S_terminal\n",
    "    S_prev = S_terminal\n",
    "\n",
    "    for i in range(n_ph-1, -1, -1):\n",
    "        Ei_b = Es[i]\n",
    "        phi_f_b = phi_fs[i]\n",
    "        Li_b = Lis[i]\n",
    "        Mi_b = Mis[i]\n",
    "        Ri_b = Ris[i]\n",
    "\n",
    "        # Build S_int batch\n",
    "        S_int = torch.zeros((B, n_x+1, n_x+1), device=device, dtype=dtype)\n",
    "        S_int[:, :n_x, :n_x] = Li_b\n",
    "\n",
    "        if n_u > 0:\n",
    "            ui_col = u_all_batch[:, i, :].view(B, n_u, 1)\n",
    "            # Mi_b: (B, n_x, n_u) -> Mi_ui: (B, n_x, 1)\n",
    "            Mi_ui = torch.matmul(Mi_b, ui_col)\n",
    "            S_int[:, :n_x, n_x:] = Mi_ui\n",
    "            S_int[:, n_x:, :n_x] = Mi_ui.transpose(-1, -2)\n",
    "            # scalar term: ui^T Ri ui -> (B,1,1)\n",
    "            tmp = torch.matmul(Ri_b, ui_col)  # (B, n_u, 1)\n",
    "            uiRiui = torch.matmul(ui_col.transpose(-1, -2), tmp)  # (B,1,1)\n",
    "            S_int[:, n_x:, n_x:] = uiRiui\n",
    "\n",
    "        # Build phi batch (B, n_x+1, n_x+1)\n",
    "        phi = torch.zeros((B, n_x+1, n_x+1), device=device, dtype=dtype)\n",
    "        phi[:, :n_x, :n_x] = Ei_b\n",
    "        phi[:, :n_x, n_x:n_x+1] = phi_f_b\n",
    "        phi[:, -1, -1] = 1.0\n",
    "\n",
    "        # S_curr = 0.5*S_int + phi^T * S_prev * phi\n",
    "        S_curr = 0.5 * S_int + torch.matmul(phi.transpose(-1, -2), torch.matmul(S_prev, phi))\n",
    "        S_list[i] = S_curr\n",
    "        S_prev = S_curr\n",
    "        \n",
    "    S0_batch = S_prev\n",
    "\n",
    "    # Compute the C and N matrices\n",
    "    C_list = []\n",
    "    N_list = []\n",
    "    \n",
    "    for i in range(n_ph):\n",
    "        A = swi.A[i].to(dtype=dtype, device=device)\n",
    "        Bmat = swi.B[i].to(dtype=dtype, device=device) if n_u > 0 else None\n",
    "        # Build batched F to preserve autograd (shape: B x (n_x+1) x (n_x+1))\n",
    "        F = torch.zeros((B, n_x+1, n_x+1), dtype=dtype, device=device)\n",
    "        # Top-left block: A (broadcasted across batch)\n",
    "        F[:, :n_x, :n_x] = A.unsqueeze(0).expand(B, n_x, n_x)\n",
    "        # Top-right column: B @ u (batched)\n",
    "        if n_u > 0:\n",
    "            ui_col = u_all_batch[:, i, :].view(B, n_u, 1)\n",
    "            F[:, :n_x, n_x:n_x+1] = torch.matmul(Bmat.unsqueeze(0), ui_col)\n",
    "        \n",
    "        # Extract the S matrix of the next phase\n",
    "        S_next = S_list[i+1]\n",
    "        H_i = Hi[i]\n",
    "        \n",
    "        # C_i: batched computation\n",
    "        Q_batch = Q.unsqueeze(0)  # (1, n_x+1, n_x+1)\n",
    "        C_i = 0.5 * Q_batch + torch.matmul(F.transpose(-2, -1), S_next) + torch.matmul(S_next, F)\n",
    "        C_list.append(C_i)\n",
    "        \n",
    "        # N matrices for each control input\n",
    "        N_i_list = []\n",
    "        for j in range(n_u):\n",
    "            Hij = H_i[:, j, :, :]  # (B, n_x+1, n_x+1)\n",
    "            # Compute N matrix\n",
    "            Nij = torch.matmul(Hij.transpose(-2, -1), S_next) + torch.matmul(S_next, Hij)\n",
    "            N_i_list.append(Nij)\n",
    "        \n",
    "        if n_u > 0:\n",
    "            N_list.append(torch.stack(N_i_list, dim=1))  # (B, n_u, n_x+1, n_x+1)\n",
    "        else:\n",
    "            N_list.append(torch.zeros((B, 0, n_x+1, n_x+1), dtype=dtype, device=device))\n",
    "    \n",
    "    # Compute gradients from C and N matrices\n",
    "    grad_u_batch = torch.zeros((B, n_ph, n_u), dtype=dtype, device=device)\n",
    "    grad_delta_batch = torch.zeros((B, n_ph), dtype=dtype, device=device)\n",
    "    \n",
    "    for i in range(n_ph):\n",
    "        C_i = C_list[i]  # (B, n_x+1, n_x+1)\n",
    "        N_i = N_list[i]  # (B, n_u, n_x+1, n_x+1)\n",
    "        D_i = D[i]  # (B, n_u, n_x+1, n_x+1)\n",
    "        \n",
    "        # Build augmented state for this phase\n",
    "        # We need to track state through phases - simplified version\n",
    "        # grad_u is derived from C matrix structure\n",
    "        if n_u > 0:\n",
    "            # Extract gradients w.r.t. controls from C matrix\n",
    "            # C has contribution from control inputs in the (n_x, n_x+1) positions\n",
    "            grad_u_batch[:, i, :] = C_i[:, :n_x, n_x].sum(dim=1, keepdim=True).expand(-1, n_u)\n",
    "            \n",
    "            # grad_delta from N and D matrices\n",
    "            # Simplified: trace of N and D contributions\n",
    "            for j in range(n_u):\n",
    "                grad_delta_batch[:, i] += (N_i[:, j] * D_i[:, j]).sum(dim=(-2, -1))\n",
    "    \n",
    "    return grad_u_batch, grad_delta_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e343963",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc013b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_analytic_gradient(\n",
    "        network: SwiLinNN,\n",
    "        X_train: torch.Tensor,\n",
    "        y_train: Optional[torch.Tensor] = None,\n",
    "        X_val: Optional[torch.Tensor] = None,\n",
    "        y_val: Optional[torch.Tensor] = None,\n",
    "        optimizer: str = 'adam',\n",
    "        learning_rate: float = 0.001,\n",
    "        weight_decay: float = 1e-4,\n",
    "        n_epochs: int = 100,\n",
    "        batch_size: int = 32,\n",
    "        device: str = 'cpu',\n",
    "        # Resampling options: regenerate new random samples every N epochs\n",
    "        resample_every: Optional[int] = None,\n",
    "        resample_fn: Optional[Callable[[int], torch.Tensor]] = None,\n",
    "        resample_val: bool = False,\n",
    "        verbose: bool = True,\n",
    "        tensorboard_logdir: Optional[str] = None,\n",
    "        log_histograms: bool = False,\n",
    "        save_history: bool = False,\n",
    "        save_history_path: Optional[str] = None,\n",
    "        save_model: bool = False,\n",
    "        save_model_path: Optional[str] = None,\n",
    "        early_stopping: bool = False,\n",
    "        early_stopping_patience: int = 20,\n",
    "        early_stopping_min_delta: float = 1e-6,\n",
    "        early_stopping_monitor: str = 'val_loss',\n",
    "    ) -> Tuple[torch.Tensor, Dict]:\n",
    "    \"\"\"\n",
    "    Train the neural network using analytic gradients\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : SwiLinNN\n",
    "        The neural network to train\n",
    "    X_train : torch.Tensor\n",
    "        Training input data\n",
    "    y_train : Optional[torch.Tensor], optional\n",
    "        Training target data, by default None\n",
    "    X_val : Optional[torch.Tensor], optional\n",
    "        Validation input data, by default None\n",
    "    y_val : Optional[torch.Tensor], optional\n",
    "        Validation target data, by default None\n",
    "    optimizer : str, optional\n",
    "        Optimizer to use, by default 'adam'\n",
    "    learning_rate : float, optional\n",
    "        Learning rate, by default 0.001\n",
    "    weight_decay : float, optional\n",
    "        Weight decay (L2 regularization), by default 1e-4\n",
    "    n_epochs : int, optional\n",
    "        Number of training epochs, by default 100\n",
    "    batch_size : int, optional\n",
    "        Batch size, by default 32\n",
    "    device : str, optional\n",
    "        Device to use ('cpu' or 'cuda'), by default 'cpu'\n",
    "    # Resampling options: regenerate new random samples every N epochs\n",
    "    resample_every : Optional[int], optional\n",
    "        Regenerate new random samples every N epochs, by default None\n",
    "    resample_fn : Optional[Callable[[int], torch.Tensor]], optional\n",
    "        Function to generate new random samples, by default None\n",
    "    resample_val : bool, optional\n",
    "        Whether to resample validation data, by default False\n",
    "    verbose : bool, optional\n",
    "        Whether to print training progress, by default True\n",
    "    tensorboard_logdir : Optional[str], optional\n",
    "        Directory for TensorBoard logs, by default None\n",
    "    log_histograms : bool, optional\n",
    "        Whether to log histograms to TensorBoard, by default False\n",
    "    save_history : bool, optional\n",
    "        Whether to save training history, by default False\n",
    "    save_history_path : Optional[str], optional\n",
    "        Path to save training history, by default None\n",
    "    save_model : bool, optional\n",
    "        Whether to save the trained model, by default False\n",
    "    save_model_path : Optional[str], optional\n",
    "        Path to save the trained model, by default None\n",
    "    early_stopping : bool, optional\n",
    "        Whether to use early stopping, by default False\n",
    "    early_stopping_patience : int, optional\n",
    "        Patience for early stopping, by default 20\n",
    "    early_stopping_min_delta : float, optional\n",
    "        Minimum delta for early stopping, by default 1e-6\n",
    "    early_stopping_monitor : str, optional\n",
    "        Metric to monitor for early stopping, by default 'val_loss'\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.Tensor, Dict]\n",
    "        The trained model and training history\n",
    "    \"\"\"\n",
    "    \n",
    "    network = network.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    \n",
    "    if X_val is not None:\n",
    "        X_val = X_val.to(device)\n",
    "\n",
    "    # Setup a default resampling function if requested but none provided.\n",
    "    # Default resampler draws uniformly between observed min/max of X_train\n",
    "    if resample_every is not None and resample_every > 0 and resample_fn is None:\n",
    "        try:\n",
    "            # x_min = float(X_train.min().item())\n",
    "            x_min = -5.0\n",
    "            # x_max = float(X_train.max().item())\n",
    "            x_max = 5.0\n",
    "        except Exception:\n",
    "            x_min, x_max = -1.0, 1.0\n",
    "\n",
    "        def _default_resample_fn(epoch, shape=X_train.shape, dtype=X_train.dtype, device_str=device, xmin=x_min, xmax=x_max):\n",
    "            # create tensor on correct device/dtype\n",
    "            dev = device_str\n",
    "            out = torch.empty(shape, dtype=dtype, device=dev).uniform_(xmin, xmax)\n",
    "            return out\n",
    "\n",
    "        resample_fn = _default_resample_fn\n",
    "    \n",
    "    n_samples = X_train.shape[0]\n",
    "    n_inputs = network.sys.n_inputs\n",
    "    \n",
    "    # Initialize PyTorch optimizer\n",
    "    if optimizer.lower() == 'adam':\n",
    "        torch_optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer.lower() == 'sgd':\n",
    "        torch_optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer.lower() == 'rmsprop':\n",
    "        torch_optimizer = torch.optim.RMSprop(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer '{optimizer}'. Supported: 'adam', 'sgd', 'rmsprop'\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        torch_optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [] if X_val is not None else None,\n",
    "        'epochs': []\n",
    "    }\n",
    "    \n",
    "    # Early stopping setup\n",
    "    if early_stopping:\n",
    "        if early_stopping_monitor == 'val_loss' and X_val is None:\n",
    "            warnings.warn(\"Early stopping monitor is 'val_loss' but no validation data provided. Switching to 'train_loss'.\")\n",
    "            early_stopping_monitor = 'train_loss'\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Early stopping enabled: monitoring '{early_stopping_monitor}' with patience={early_stopping_patience}, min_delta={early_stopping_min_delta}\")\n",
    "    \n",
    "    # Setup TensorBoard writer if requested\n",
    "    writer = SummaryWriter(log_dir=tensorboard_logdir) if tensorboard_logdir is not None else None\n",
    "\n",
    "    # Determine history save path\n",
    "    if save_history:\n",
    "        if save_history_path is None:\n",
    "            if tensorboard_logdir is not None:\n",
    "                save_history_path = os.path.join(tensorboard_logdir, 'history.json')\n",
    "            else:\n",
    "                save_history_path = os.path.join(os.getcwd(), 'training_history.json')\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        # Optionally resample training (and validation) data every `resample_every` epochs\n",
    "        if resample_every is not None and resample_every > 0 and epoch > 0 and (epoch % resample_every) == 0:\n",
    "            if resample_fn is None:\n",
    "                warnings.warn(\"resample_every set but resample_fn is None; skipping resampling.\")\n",
    "            else:\n",
    "                try:\n",
    "                    new_data = resample_fn(epoch)\n",
    "                    # support returning either X_train or (X_train, X_val)\n",
    "                    if isinstance(new_data, (list, tuple)) and len(new_data) == 2:\n",
    "                        new_X_train, new_X_val = new_data\n",
    "                    else:\n",
    "                        new_X_train, new_X_val = new_data, None\n",
    "\n",
    "                    if not torch.is_tensor(new_X_train):\n",
    "                        new_X_train = torch.as_tensor(new_X_train)\n",
    "                    X_train = new_X_train.to(device)\n",
    "                    n_samples = X_train.shape[0]\n",
    "\n",
    "                    if resample_val and new_X_val is not None:\n",
    "                        if not torch.is_tensor(new_X_val):\n",
    "                            new_X_val = torch.as_tensor(new_X_val)\n",
    "                        X_val = new_X_val.to(device)\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"Resampled training data at epoch {epoch + 1}\")\n",
    "                except Exception as e:\n",
    "                    warnings.warn(f\"Resampling failed at epoch {epoch + 1}: {e}\")\n",
    "\n",
    "        # Create random batches\n",
    "        indices = torch.randperm(n_samples, device=device)\n",
    "        \n",
    "        for start_idx in range(0, n_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            X_batch = X_train[batch_indices]\n",
    "            current_batch_size = X_batch.shape[0]\n",
    "            \n",
    "            # Zero gradients\n",
    "            torch_optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = network(X_batch)\n",
    "            \n",
    "            # Apply transformation: T * softmax(output[-n_phases:]) for the deltas\n",
    "            T_tensor = torch.tensor(network.sys.time_horizon, device=output.device, dtype=output.dtype)\n",
    "\n",
    "            # Handle batch dimension properly\n",
    "            n_control_outputs = network.n_phases * n_inputs\n",
    "            controls = output[:, :n_control_outputs] # shape (batch_size, n_phases * n_inputs)\n",
    "            delta_raw = output[:, n_control_outputs:]\n",
    "            \n",
    "            # To build a diffeomorphism, we fix the value of one delta_raw to zero\n",
    "            # This keeps the positivity and sum-to-T properties validity\n",
    "            # make the first delta value identically zero while preserving gradients\n",
    "            last = delta_raw[:, -1:]  # shape (batch_size, 1)\n",
    "            delta_raw_traslated = delta_raw - last  # subtract broadcasted last column -> last becomes 0 (differentiable)\n",
    "            \n",
    "            # Apply softmax and scale deltas\n",
    "            delta_normalized = F.softmax(delta_raw_traslated, dim=-1)\n",
    "            deltas = delta_normalized * T_tensor # shape (batch_size, n_phases)\n",
    "            \n",
    "            # Clip controls using tanh-based soft clipping to preserve gradients\n",
    "            u_min = -1.0  # Define your lower bound\n",
    "            u_max = 1.0   # Define your upper bound\n",
    "            u_center = (u_max + u_min) / 2.0\n",
    "            u_range = (u_max - u_min) / 2.0\n",
    "            # Soft clipping: maps (-inf, inf) to (u_min, u_max) smoothly\n",
    "            controls = u_center + u_range * torch.tanh(controls)\n",
    "            \n",
    "            transformed_output = torch.cat([controls, deltas], dim=-1) # shape (batch_size, n_phases * (n_inputs + 1))\n",
    "\n",
    "            # Vectorized batch loss computation\n",
    "            # reshape controls to (B, n_phases, n_inputs)\n",
    "            B_batch = current_batch_size\n",
    "            controls_reshaped = controls.view(B_batch, network.n_phases, n_inputs)\n",
    "            deltas_batch = deltas.view(B_batch, network.n_phases)\n",
    "            x0_batch = X_batch\n",
    "\n",
    "            J_batch = evaluate_cost_functional_batch(network.sys, controls_reshaped, deltas_batch, x0_batch)\n",
    "            loss = J_batch.mean()\n",
    "            \n",
    "            # Backward pass\n",
    "            # Compute Jacobian: derivative of each output w.r.t. parameters\n",
    "            # Sum the output over the batch dimension to get a scalar loss\n",
    "            jacobian = []\n",
    "            for i in range(output.shape[1] if output.dim() > 1 else 1):\n",
    "                network.zero_grad()\n",
    "                if output.dim() > 1:\n",
    "                    output[:, i].sum().backward(retain_graph=True)\n",
    "                else:\n",
    "                    output.sum().backward(retain_graph=True)\n",
    "                \n",
    "                grads_i = []\n",
    "                for param in network.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        grads_i.append(param.grad.view(-1).clone())\n",
    "                jacobian.append(torch.cat(grads_i))\n",
    "\n",
    "            jacobian = torch.stack(jacobian)  # Shape: (n_outputs, n_params)\n",
    "            \n",
    "            # Include the derivative of the loss w.r.t. the outputs of the NN\n",
    "            # This is computed via the analytic gradient function\n",
    "            grad_u_batch, grad_delta_batch = evaluate_gradient_batch(\n",
    "                network.sys,\n",
    "                controls_reshaped,\n",
    "                deltas_batch,\n",
    "                x0_batch\n",
    "            )  # Shapes: (B, n_phases, n_inputs), (B, n_phases)\n",
    "            \n",
    "            # Combine gradients into shape (B, n_outputs)\n",
    "            grad_output_batch = torch.cat([\n",
    "                grad_u_batch.view(B_batch, -1),\n",
    "                grad_delta_batch\n",
    "            ], dim=-1)  # Shape: (B, n_outputs)\n",
    "            \n",
    "            # Compute gradient of loss w.r.t. parameters via chain rule\n",
    "            grad_params = torch.zeros(jacobian.shape[1], device=device, dtype=jacobian.dtype)\n",
    "            for b in range(B_batch):\n",
    "                grad_params += (grad_output_batch[b].unsqueeze(0) @ jacobian).squeeze(0)\n",
    "            grad_params /= B_batch  # Average over batch\n",
    "            # Compute gradient norm for logging\n",
    "            grad_norm = None\n",
    "            if writer is not None:\n",
    "                tot = torch.tensor(0.0, device=device)\n",
    "                for p in network.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        tot = tot + p.grad.detach().to(device).pow(2).sum()\n",
    "                grad_norm = torch.sqrt(tot).item()\n",
    "\n",
    "            # Optimizer step\n",
    "            torch_optimizer.step()\n",
    "\n",
    "            # Log per-batch stats to TensorBoard (optional)\n",
    "            if writer is not None:\n",
    "                global_step = epoch * max(1, n_samples // batch_size) + n_batches\n",
    "                writer.add_scalar('train/batch_loss', loss.item(), global_step)\n",
    "                if grad_norm is not None:\n",
    "                    writer.add_scalar('train/batch_grad_norm', grad_norm, global_step)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        avg_train_loss = epoch_loss / n_batches\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['epochs'].append(epoch)\n",
    "        \n",
    "        # Validation loss\n",
    "        if X_val is not None:\n",
    "            with torch.no_grad():\n",
    "                val_output = network(X_val)\n",
    "                \n",
    "                # Transform validation output\n",
    "                n_control_outputs = network.n_phases * n_inputs\n",
    "                val_controls = val_output[:, :n_control_outputs]\n",
    "                # Clip controls using tanh-based soft clipping to preserve gradients\n",
    "                u_min = -1.0  # Define your lower bound\n",
    "                u_max = 1.0   # Define your upper bound\n",
    "                u_center = (u_max + u_min) / 2.0\n",
    "                u_range = (u_max - u_min) / 2.0\n",
    "                # Soft clipping: maps (-inf, inf) to (u_min, u_max) smoothly\n",
    "                val_controls = u_center + u_range * torch.tanh(val_controls)\n",
    "                val_delta_raw = val_output[:, n_control_outputs:]\n",
    "                val_delta_raw_last = val_delta_raw[:, -1:]\n",
    "                val_delta_raw_traslated = val_delta_raw - val_delta_raw_last\n",
    "                val_delta_normalized = F.softmax(val_delta_raw_traslated, dim=-1)\n",
    "                val_deltas = val_delta_normalized * T_tensor\n",
    "                val_transformed = torch.cat([val_controls, val_deltas], dim=-1)\n",
    "                \n",
    "                # Vectorized validation loss\n",
    "                Bv = X_val.shape[0]\n",
    "                val_controls = val_controls.view(Bv, network.n_phases, n_inputs)\n",
    "                val_deltas = val_deltas.view(Bv, network.n_phases)\n",
    "                J_val = evaluate_cost_functional_batch(network.sys, val_controls, val_deltas, X_val)\n",
    "                avg_val_loss = J_val.mean().item()\n",
    "                history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        if X_val is not None:\n",
    "            scheduler.step(avg_val_loss)\n",
    "        else:\n",
    "            scheduler.step(avg_train_loss)\n",
    "\n",
    "        # Write epoch-level scalars to TensorBoard\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('train/epoch_loss', avg_train_loss, epoch)\n",
    "            writer.add_scalar('train/learning_rate', torch_optimizer.param_groups[0]['lr'], epoch)\n",
    "            if X_val is not None:\n",
    "                writer.add_scalar('val/epoch_loss', avg_val_loss, epoch)\n",
    "            # Optionally log parameter histograms once per epoch\n",
    "            if log_histograms:\n",
    "                for name, param in network.named_parameters():\n",
    "                    writer.add_histogram(f'params/{name}', param.detach().cpu().numpy(), epoch)\n",
    "\n",
    "        # Save history to disk each epoch if requested\n",
    "        if save_history:\n",
    "            try:\n",
    "                serial = {}\n",
    "                for k, v in history.items():\n",
    "                    if v is None:\n",
    "                        serial[k] = None\n",
    "                    elif isinstance(v, list):\n",
    "                        serial[k] = [float(x) for x in v]\n",
    "                    else:\n",
    "                        serial[k] = v\n",
    "                # Ensure directory exists\n",
    "                os.makedirs(os.path.dirname(save_history_path), exist_ok=True)\n",
    "                with open(save_history_path, 'w') as fh:\n",
    "                    json.dump(serial, fh, indent=2)\n",
    "            except Exception:\n",
    "                # Don't interrupt training on save failure; warn instead\n",
    "                warnings.warn(f\"Failed to save training history to {save_history_path}\")\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (epoch + 1) % max(1, n_epochs // 10) == 0:\n",
    "            if X_val is not None:\n",
    "                print(f\"Epoch {epoch + 1}/{n_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch + 1}/{n_epochs} - Train Loss: {avg_train_loss:.6f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping:\n",
    "            # Determine which loss to monitor\n",
    "            current_loss = avg_val_loss if early_stopping_monitor == 'val_loss' else avg_train_loss\n",
    "            \n",
    "            # Check if there's improvement\n",
    "            if current_loss < best_loss - early_stopping_min_delta:\n",
    "                best_loss = current_loss\n",
    "                best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "                # Save best model state\n",
    "                best_model_state = {k: v.cpu().clone() for k, v in network.state_dict().items()}\n",
    "                if verbose and epoch > 0:\n",
    "                    print(f\"   New best {early_stopping_monitor}: {best_loss:.6f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if verbose and patience_counter > 0 and (epoch + 1) % max(1, n_epochs // 10) == 0:\n",
    "                    print(f\"   No improvement for {patience_counter} epoch(s)\")\n",
    "            \n",
    "            # Check if we should stop\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                if verbose:\n",
    "                    print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "                    print(f\"Best {early_stopping_monitor}: {best_loss:.6f} at epoch {best_epoch + 1}\")\n",
    "                \n",
    "                # Restore best model state\n",
    "                if best_model_state is not None:\n",
    "                    network.load_state_dict(best_model_state)\n",
    "                    if verbose:\n",
    "                        print(\"Restored best model weights\")\n",
    "                \n",
    "                break\n",
    "    \n",
    "    # Get final parameters\n",
    "    params_optimized = network.get_flat_params()\n",
    "    \n",
    "    # Optionally save the trained model parameters\n",
    "    if save_model:\n",
    "        if save_model_path is None:\n",
    "            if tensorboard_logdir is not None:\n",
    "                save_model_path = os.path.join(tensorboard_logdir, 'model_state_dict.pt')\n",
    "            else:\n",
    "                save_model_path = os.path.join(os.getcwd(), 'model_state_dict.pt')\n",
    "        try:\n",
    "            network.save(save_model_path)\n",
    "            if verbose:\n",
    "                print(f\"Saved model state_dict to: {save_model_path}\")\n",
    "        except Exception:\n",
    "            warnings.warn(f\"Failed to save model to {save_model_path}\")\n",
    "\n",
    "    # Add early stopping info to history\n",
    "    if early_stopping:\n",
    "        history['early_stopping'] = {\n",
    "            'triggered': patience_counter >= early_stopping_patience,\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_loss': best_loss,\n",
    "            'monitored_metric': early_stopping_monitor,\n",
    "            'patience': early_stopping_patience,\n",
    "            'final_epoch': epoch\n",
    "        }\n",
    "\n",
    "    # Print final losses\n",
    "    if verbose:\n",
    "        print(f\"\\nFinal Training Loss: {history['train_loss'][-1]:.6f}\")\n",
    "        if X_val is not None and history['val_loss']:\n",
    "            print(f\"Final Validation Loss: {history['val_loss'][-1]:.6f}\")\n",
    "        if early_stopping and history.get('early_stopping', {}).get('triggered', False):\n",
    "            print(f\"\\nEarly stopping was triggered:\")\n",
    "            print(f\"  Best {early_stopping_monitor}: {best_loss:.6f} at epoch {best_epoch + 1}\")\n",
    "            print(f\"  Training stopped at epoch {epoch + 1}\")\n",
    "\n",
    "    return params_optimized, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9200f4cf",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bca1fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Example: Neural Network Training\n",
      "======================================================================\n",
      "Using device: cuda\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Example: Neural Network Training\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate synthetic data\n",
    "torch.manual_seed(42)\n",
    "n_samples_train = 100\n",
    "n_samples_val = 20\n",
    "n_phases = 10\n",
    "n_control_inputs = 1\n",
    "n_NN_inputs = 3\n",
    "n_NN_outputs = n_phases * (n_control_inputs + 1)\n",
    "\n",
    "X_train = torch.empty(n_samples_train, n_NN_inputs).uniform_(-1.0, 1.0)\n",
    "\n",
    "\n",
    "X_val = torch.empty(n_samples_val, n_NN_inputs).uniform_(-1.0, 1.0)\n",
    "\n",
    "# Create network\n",
    "network = SwiLinNN(\n",
    "    layer_sizes=[n_NN_inputs, 50, 50, n_NN_outputs],\n",
    "    n_phases=n_phases,\n",
    "    activation='relu',\n",
    "    output_activation='linear'\n",
    ")\n",
    "\n",
    "# Train\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Store the path where the script is located\n",
    "# In a Jupyter notebook __file__ is not defined, fall back to the current working directory\n",
    "try:\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    script_dir = os.getcwd()\n",
    "\n",
    "date = subprocess.check_output(['date', '+%Y%m%d_%H%M%S']).decode('utf-8').strip()\n",
    "tensorboard_logdir = os.path.join(script_dir, \"..\", \"logs\", date)\n",
    "model_name = f\"example_50_50_torch_{date}.pt\"\n",
    "models_dir = os.path.join(script_dir, \"..\", \"models\", model_name)\n",
    "\n",
    "params_opt, history = train_neural_network_analytic_gradient(\n",
    "    network=network,\n",
    "    X_train=X_train,\n",
    "    # y_train=None,\n",
    "    X_val=X_val,\n",
    "    # y_val=None,\n",
    "    optimizer='adam',\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-4,\n",
    "    n_epochs=400,\n",
    "    resample_every=None,\n",
    "    resample_fn=None,\n",
    "    resample_val=False,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=30,\n",
    "    early_stopping_min_delta=1e-4,\n",
    "    batch_size=n_samples_train,\n",
    "    device=device,\n",
    "    verbose=False,\n",
    "    tensorboard_logdir=tensorboard_logdir,\n",
    "    log_histograms=False,\n",
    "    save_model=True,\n",
    "    save_model_path=models_dir\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0440bf4",
   "metadata": {},
   "source": [
    "## Explanation of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb39a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Load data from file <filename>, which has to be in the data folder.\n",
    "    The function loads both csv or mat files\n",
    "    \"\"\"\n",
    "    data_folder = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'data'))\n",
    "    file_path = os.path.join(data_folder, filename)\n",
    "\n",
    "    if filename.endswith('.csv'):\n",
    "        loaded_data = np.loadtxt(file_path, delimiter=',')\n",
    "    elif filename.endswith('.mat'):\n",
    "        loaded_data = scipy.io.loadmat(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use .csv or .mat files.\")\n",
    "    \n",
    "    # Handle data after loading\n",
    "    keys_to_keep = ['n_phases', 'controls', 'phases_duration']\n",
    "\n",
    "    data = {k: loaded_data[k] for k in keys_to_keep}\n",
    "    # Normalize and reshape controls into shape (n_inputs, n_phases)\n",
    "    controls = np.asarray(data['controls']).ravel()\n",
    "    n_phases = int(np.squeeze(np.asarray(data['n_phases'])))\n",
    "    if controls.size % n_phases != 0:\n",
    "        raise ValueError(f\"Controls length ({controls.size}) is not divisible by n_phases ({n_phases}).\")\n",
    "    n_inputs = controls.size // n_phases\n",
    "    controls = controls.reshape((n_inputs, n_phases))\n",
    "    data['n_inputs'] = n_inputs\n",
    "\n",
    "    # Ensure phases_duration is a 1D array\n",
    "    data['phases_duration'] = np.asarray(data['phases_duration']).ravel()\n",
    "\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
