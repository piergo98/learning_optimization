{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3520cd24",
   "metadata": {},
   "source": [
    "# The aim of this file is to inspect the behavior of the NN's params with respect to the gradient of the cost function w.r.t. u and $\\delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, subprocess, sys\n",
    "import scipy.io\n",
    "from scipy.linalg import solve_continuous_are\n",
    "from scipy.special import softmax\n",
    "from typing import Optional, Callable, Tuple, Dict, List\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    warnings.warn(\"PyTorch not available. GPU training will not be available.\")\n",
    "    \n",
    "from ocslc.switched_linear_mpc import SwitchedLinearMPC as SwiLin_casadi\n",
    "\n",
    "from src.switched_linear_torch import SwiLin\n",
    "from src.training import SwiLinNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de771d10",
   "metadata": {},
   "source": [
    "## Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4733132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings\n",
    "N_PHASES = 80\n",
    "TIME_HORIZON = 10.0\n",
    "\n",
    "# NN settings\n",
    "N_CONTROL_INPUTS = 1\n",
    "N_STATES = 3\n",
    "N_NN_INPUTS = 3\n",
    "N_NN_OUTPUTS = N_PHASES * (N_CONTROL_INPUTS + 1)  # +1 for the mode\n",
    "\n",
    "# Casadi settings\n",
    "MULTIPLE_SHOOTING = True\n",
    "INTEGRATOR = 'exp'\n",
    "HYBRID = False\n",
    "PLOT = 'display'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e779023",
   "metadata": {},
   "source": [
    "## Compute cost gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient function\n",
    "def gradient_func(params, indices=None, data=None):\n",
    "    network.set_flat_params(params)\n",
    "    network.zero_grad()\n",
    "    \n",
    "    if indices is not None:\n",
    "        X_batch = X_train[indices]\n",
    "        y_batch = y_train[indices]\n",
    "    else:\n",
    "        X_batch = X_train\n",
    "        y_batch = y_train\n",
    "    \n",
    "    output = network(X_batch)\n",
    "    \n",
    "    # Compute Jacobian: derivative of each output w.r.t. parameters\n",
    "    # Sum the output over the batch dimension to get a scalar loss\n",
    "    jacobian = []\n",
    "    for i in range(output.shape[1] if output.dim() > 1 else 1):\n",
    "        network.zero_grad()\n",
    "        if output.dim() > 1:\n",
    "            output[:, i].sum().backward(retain_graph=True)\n",
    "        else:\n",
    "            output.sum().backward(retain_graph=True)\n",
    "        \n",
    "        grads_i = []\n",
    "        for param in network.parameters():\n",
    "            if param.grad is not None:\n",
    "                grads_i.append(param.grad.view(-1).clone())\n",
    "        jacobian.append(torch.cat(grads_i))\n",
    "\n",
    "    jacobian = torch.stack(jacobian)  # Shape: (n_outputs, n_params)\n",
    "    \n",
    "    # Include the derivative of the loss w.r.t. the outputs of the NN\n",
    "    \n",
    "    if network.output_activation == 'softmax' and y_batch.dtype == torch.long:\n",
    "        loss = criterion(output, y_batch)\n",
    "    else:\n",
    "        loss = criterion(output, y_batch)\n",
    "    \n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    print(f\"Loss shape: {loss.shape}\")\n",
    "    input(\"Press Enter to continue...\")\n",
    "    loss.backward()\n",
    "    \n",
    "    # Collect gradients\n",
    "    grads = []\n",
    "    for param in network.parameters():\n",
    "        if param.grad is not None:\n",
    "            grads.append(param.grad.view(-1))\n",
    "\n",
    "    pippo = torch.cat(grads)\n",
    "    print(f\"Gradient shape: {pippo.shape}\")\n",
    "    input(\"Press Enter to continue...\")\n",
    "    return torch.cat(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edf5e65",
   "metadata": {},
   "source": [
    "## Train Neural Network with Analytic Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc013b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_analytic_gradient(\n",
    "        network: SwiLinNN,\n",
    "        X_train: torch.Tensor,\n",
    "        y_train: Optional[torch.Tensor] = None,\n",
    "        X_val: Optional[torch.Tensor] = None,\n",
    "        y_val: Optional[torch.Tensor] = None,\n",
    "        optimizer: str = 'adam',\n",
    "        learning_rate: float = 0.001,\n",
    "        weight_decay: float = 1e-4,\n",
    "        n_epochs: int = 100,\n",
    "        batch_size: int = 32,\n",
    "        device: str = 'cpu',\n",
    "        # Resampling options: regenerate new random samples every N epochs\n",
    "        resample_every: Optional[int] = None,\n",
    "        resample_fn: Optional[Callable[[int], torch.Tensor]] = None,\n",
    "        resample_val: bool = False,\n",
    "        verbose: bool = True,\n",
    "        tensorboard_logdir: Optional[str] = None,\n",
    "        log_histograms: bool = False,\n",
    "        save_history: bool = False,\n",
    "        save_history_path: Optional[str] = None,\n",
    "        save_model: bool = False,\n",
    "        save_model_path: Optional[str] = None,\n",
    "        early_stopping: bool = False,\n",
    "        early_stopping_patience: int = 20,\n",
    "        early_stopping_min_delta: float = 1e-6,\n",
    "        early_stopping_monitor: str = 'val_loss',\n",
    "    ) -> Tuple[torch.Tensor, Dict]:\n",
    "    \"\"\"\n",
    "    Train the neural network using analytic gradients\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : SwiLinNN\n",
    "        The neural network to train\n",
    "    X_train : torch.Tensor\n",
    "        Training input data\n",
    "    y_train : Optional[torch.Tensor], optional\n",
    "        Training target data, by default None\n",
    "    X_val : Optional[torch.Tensor], optional\n",
    "        Validation input data, by default None\n",
    "    y_val : Optional[torch.Tensor], optional\n",
    "        Validation target data, by default None\n",
    "    optimizer : str, optional\n",
    "        Optimizer to use, by default 'adam'\n",
    "    learning_rate : float, optional\n",
    "        Learning rate, by default 0.001\n",
    "    weight_decay : float, optional\n",
    "        Weight decay (L2 regularization), by default 1e-4\n",
    "    n_epochs : int, optional\n",
    "        Number of training epochs, by default 100\n",
    "    batch_size : int, optional\n",
    "        Batch size, by default 32\n",
    "    device : str, optional\n",
    "        Device to use ('cpu' or 'cuda'), by default 'cpu'\n",
    "    # Resampling options: regenerate new random samples every N epochs\n",
    "    resample_every : Optional[int], optional\n",
    "        Regenerate new random samples every N epochs, by default None\n",
    "    resample_fn : Optional[Callable[[int], torch.Tensor]], optional\n",
    "        Function to generate new random samples, by default None\n",
    "    resample_val : bool, optional\n",
    "        Whether to resample validation data, by default False\n",
    "    verbose : bool, optional\n",
    "        Whether to print training progress, by default True\n",
    "    tensorboard_logdir : Optional[str], optional\n",
    "        Directory for TensorBoard logs, by default None\n",
    "    log_histograms : bool, optional\n",
    "        Whether to log histograms to TensorBoard, by default False\n",
    "    save_history : bool, optional\n",
    "        Whether to save training history, by default False\n",
    "    save_history_path : Optional[str], optional\n",
    "        Path to save training history, by default None\n",
    "    save_model : bool, optional\n",
    "        Whether to save the trained model, by default False\n",
    "    save_model_path : Optional[str], optional\n",
    "        Path to save the trained model, by default None\n",
    "    early_stopping : bool, optional\n",
    "        Whether to use early stopping, by default False\n",
    "    early_stopping_patience : int, optional\n",
    "        Patience for early stopping, by default 20\n",
    "    early_stopping_min_delta : float, optional\n",
    "        Minimum delta for early stopping, by default 1e-6\n",
    "    early_stopping_monitor : str, optional\n",
    "        Metric to monitor for early stopping, by default 'val_loss'\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.Tensor, Dict]\n",
    "        The trained model and training history\n",
    "    \"\"\"\n",
    "    \n",
    "    network = network.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    \n",
    "    if X_val is not None:\n",
    "        X_val = X_val.to(device)\n",
    "\n",
    "    # Setup a default resampling function if requested but none provided.\n",
    "    # Default resampler draws uniformly between observed min/max of X_train\n",
    "    if resample_every is not None and resample_every > 0 and resample_fn is None:\n",
    "        try:\n",
    "            # x_min = float(X_train.min().item())\n",
    "            x_min = -5.0\n",
    "            # x_max = float(X_train.max().item())\n",
    "            x_max = 5.0\n",
    "        except Exception:\n",
    "            x_min, x_max = -1.0, 1.0\n",
    "\n",
    "        def _default_resample_fn(epoch, shape=X_train.shape, dtype=X_train.dtype, device_str=device, xmin=x_min, xmax=x_max):\n",
    "            # create tensor on correct device/dtype\n",
    "            dev = device_str\n",
    "            out = torch.empty(shape, dtype=dtype, device=dev).uniform_(xmin, xmax)\n",
    "            return out\n",
    "\n",
    "        resample_fn = _default_resample_fn\n",
    "    \n",
    "    n_samples = X_train.shape[0]\n",
    "    n_inputs = network.sys.n_inputs\n",
    "    \n",
    "    # Initialize PyTorch optimizer\n",
    "    if optimizer.lower() == 'adam':\n",
    "        torch_optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer.lower() == 'sgd':\n",
    "        torch_optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer.lower() == 'rmsprop':\n",
    "        torch_optimizer = torch.optim.RMSprop(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer '{optimizer}'. Supported: 'adam', 'sgd', 'rmsprop'\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        torch_optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [] if X_val is not None else None,\n",
    "        'epochs': []\n",
    "    }\n",
    "    \n",
    "    # Early stopping setup\n",
    "    if early_stopping:\n",
    "        if early_stopping_monitor == 'val_loss' and X_val is None:\n",
    "            warnings.warn(\"Early stopping monitor is 'val_loss' but no validation data provided. Switching to 'train_loss'.\")\n",
    "            early_stopping_monitor = 'train_loss'\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Early stopping enabled: monitoring '{early_stopping_monitor}' with patience={early_stopping_patience}, min_delta={early_stopping_min_delta}\")\n",
    "    \n",
    "    # Setup TensorBoard writer if requested\n",
    "    writer = SummaryWriter(log_dir=tensorboard_logdir) if tensorboard_logdir is not None else None\n",
    "\n",
    "    # Determine history save path\n",
    "    if save_history:\n",
    "        if save_history_path is None:\n",
    "            if tensorboard_logdir is not None:\n",
    "                save_history_path = os.path.join(tensorboard_logdir, 'history.json')\n",
    "            else:\n",
    "                save_history_path = os.path.join(os.getcwd(), 'training_history.json')\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        # Optionally resample training (and validation) data every `resample_every` epochs\n",
    "        if resample_every is not None and resample_every > 0 and epoch > 0 and (epoch % resample_every) == 0:\n",
    "            if resample_fn is None:\n",
    "                warnings.warn(\"resample_every set but resample_fn is None; skipping resampling.\")\n",
    "            else:\n",
    "                try:\n",
    "                    new_data = resample_fn(epoch)\n",
    "                    # support returning either X_train or (X_train, X_val)\n",
    "                    if isinstance(new_data, (list, tuple)) and len(new_data) == 2:\n",
    "                        new_X_train, new_X_val = new_data\n",
    "                    else:\n",
    "                        new_X_train, new_X_val = new_data, None\n",
    "\n",
    "                    if not torch.is_tensor(new_X_train):\n",
    "                        new_X_train = torch.as_tensor(new_X_train)\n",
    "                    X_train = new_X_train.to(device)\n",
    "                    n_samples = X_train.shape[0]\n",
    "\n",
    "                    if resample_val and new_X_val is not None:\n",
    "                        if not torch.is_tensor(new_X_val):\n",
    "                            new_X_val = torch.as_tensor(new_X_val)\n",
    "                        X_val = new_X_val.to(device)\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"Resampled training data at epoch {epoch + 1}\")\n",
    "                except Exception as e:\n",
    "                    warnings.warn(f\"Resampling failed at epoch {epoch + 1}: {e}\")\n",
    "\n",
    "        # Create random batches\n",
    "        indices = torch.randperm(n_samples, device=device)\n",
    "        \n",
    "        for start_idx in range(0, n_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            X_batch = X_train[batch_indices]\n",
    "            current_batch_size = X_batch.shape[0]\n",
    "            \n",
    "            # Zero gradients\n",
    "            torch_optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = network(X_batch)\n",
    "            \n",
    "            # Apply transformation: T * softmax(output[-n_phases:]) for the deltas\n",
    "            T_tensor = torch.tensor(network.sys.time_horizon, device=output.device, dtype=output.dtype)\n",
    "\n",
    "            # Handle batch dimension properly\n",
    "            n_control_outputs = network.n_phases * n_inputs\n",
    "            controls = output[:, :n_control_outputs] # shape (batch_size, n_phases * n_inputs)\n",
    "            delta_raw = output[:, n_control_outputs:]\n",
    "            \n",
    "            # To build a diffeomorphism, we fix the value of one delta_raw to zero\n",
    "            # This keeps the positivity and sum-to-T properties validity\n",
    "            # make the first delta value identically zero while preserving gradients\n",
    "            last = delta_raw[:, -1:]  # shape (batch_size, 1)\n",
    "            delta_raw_traslated = delta_raw - last  # subtract broadcasted last column -> last becomes 0 (differentiable)\n",
    "            \n",
    "            # Apply softmax and scale deltas\n",
    "            delta_normalized = F.softmax(delta_raw_traslated, dim=-1)\n",
    "            deltas = delta_normalized * T_tensor # shape (batch_size, n_phases)\n",
    "            \n",
    "            # Clip controls using tanh-based soft clipping to preserve gradients\n",
    "            u_min = -1.0  # Define your lower bound\n",
    "            u_max = 1.0   # Define your upper bound\n",
    "            u_center = (u_max + u_min) / 2.0\n",
    "            u_range = (u_max - u_min) / 2.0\n",
    "            # Soft clipping: maps (-inf, inf) to (u_min, u_max) smoothly\n",
    "            controls = u_center + u_range * torch.tanh(controls)\n",
    "            \n",
    "            transformed_output = torch.cat([controls, deltas], dim=-1) # shape (batch_size, n_phases * (n_inputs + 1))\n",
    "            \n",
    "            # Instead of the for loop, I have to give the full batch to the cost function\n",
    "\n",
    "            # Vectorized batch loss computation\n",
    "            # reshape controls to (B, n_phases, n_inputs)\n",
    "            B_batch = current_batch_size\n",
    "            controls_reshaped = controls.view(B_batch, network.n_phases, n_inputs)\n",
    "            deltas_batch = deltas.view(B_batch, network.n_phases)\n",
    "            x0_batch = X_batch\n",
    "\n",
    "            J_batch = evaluate_cost_functional_batch(network.sys, controls_reshaped, deltas_batch, x0_batch)\n",
    "            loss = J_batch.mean()\n",
    "            \n",
    "            # Backward pass\n",
    "            # TODO: substitute this with the analytic gradient computation and the matrix from the NN\n",
    "            loss.backward()\n",
    "            # Compute gradient norm for logging\n",
    "            grad_norm = None\n",
    "            if writer is not None:\n",
    "                tot = torch.tensor(0.0, device=device)\n",
    "                for p in network.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        tot = tot + p.grad.detach().to(device).pow(2).sum()\n",
    "                grad_norm = torch.sqrt(tot).item()\n",
    "\n",
    "            # Optimizer step\n",
    "            torch_optimizer.step()\n",
    "\n",
    "            # Log per-batch stats to TensorBoard (optional)\n",
    "            if writer is not None:\n",
    "                global_step = epoch * max(1, n_samples // batch_size) + n_batches\n",
    "                writer.add_scalar('train/batch_loss', loss.item(), global_step)\n",
    "                if grad_norm is not None:\n",
    "                    writer.add_scalar('train/batch_grad_norm', grad_norm, global_step)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        avg_train_loss = epoch_loss / n_batches\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['epochs'].append(epoch)\n",
    "        \n",
    "        # Validation loss\n",
    "        if X_val is not None:\n",
    "            with torch.no_grad():\n",
    "                val_output = network(X_val)\n",
    "                \n",
    "                # Transform validation output\n",
    "                n_control_outputs = network.n_phases * n_inputs\n",
    "                val_controls = val_output[:, :n_control_outputs]\n",
    "                # Clip controls using tanh-based soft clipping to preserve gradients\n",
    "                u_min = -1.0  # Define your lower bound\n",
    "                u_max = 1.0   # Define your upper bound\n",
    "                u_center = (u_max + u_min) / 2.0\n",
    "                u_range = (u_max - u_min) / 2.0\n",
    "                # Soft clipping: maps (-inf, inf) to (u_min, u_max) smoothly\n",
    "                val_controls = u_center + u_range * torch.tanh(val_controls)\n",
    "                val_delta_raw = val_output[:, n_control_outputs:]\n",
    "                val_delta_raw_last = val_delta_raw[:, -1:]\n",
    "                val_delta_raw_traslated = val_delta_raw - val_delta_raw_last\n",
    "                val_delta_normalized = F.softmax(val_delta_raw_traslated, dim=-1)\n",
    "                val_deltas = val_delta_normalized * T_tensor\n",
    "                val_transformed = torch.cat([val_controls, val_deltas], dim=-1)\n",
    "                \n",
    "                # Vectorized validation loss\n",
    "                Bv = X_val.shape[0]\n",
    "                val_controls = val_controls.view(Bv, network.n_phases, n_inputs)\n",
    "                val_deltas = val_deltas.view(Bv, network.n_phases)\n",
    "                J_val = evaluate_cost_functional_batch(network.sys, val_controls, val_deltas, X_val)\n",
    "                avg_val_loss = J_val.mean().item()\n",
    "                history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        if X_val is not None:\n",
    "            scheduler.step(avg_val_loss)\n",
    "        else:\n",
    "            scheduler.step(avg_train_loss)\n",
    "\n",
    "        # Write epoch-level scalars to TensorBoard\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('train/epoch_loss', avg_train_loss, epoch)\n",
    "            writer.add_scalar('train/learning_rate', torch_optimizer.param_groups[0]['lr'], epoch)\n",
    "            if X_val is not None:\n",
    "                writer.add_scalar('val/epoch_loss', avg_val_loss, epoch)\n",
    "            # Optionally log parameter histograms once per epoch\n",
    "            if log_histograms:\n",
    "                for name, param in network.named_parameters():\n",
    "                    writer.add_histogram(f'params/{name}', param.detach().cpu().numpy(), epoch)\n",
    "\n",
    "        # Save history to disk each epoch if requested\n",
    "        if save_history:\n",
    "            try:\n",
    "                serial = {}\n",
    "                for k, v in history.items():\n",
    "                    if v is None:\n",
    "                        serial[k] = None\n",
    "                    elif isinstance(v, list):\n",
    "                        serial[k] = [float(x) for x in v]\n",
    "                    else:\n",
    "                        serial[k] = v\n",
    "                # Ensure directory exists\n",
    "                os.makedirs(os.path.dirname(save_history_path), exist_ok=True)\n",
    "                with open(save_history_path, 'w') as fh:\n",
    "                    json.dump(serial, fh, indent=2)\n",
    "            except Exception:\n",
    "                # Don't interrupt training on save failure; warn instead\n",
    "                warnings.warn(f\"Failed to save training history to {save_history_path}\")\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (epoch + 1) % max(1, n_epochs // 10) == 0:\n",
    "            if X_val is not None:\n",
    "                print(f\"Epoch {epoch + 1}/{n_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch + 1}/{n_epochs} - Train Loss: {avg_train_loss:.6f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping:\n",
    "            # Determine which loss to monitor\n",
    "            current_loss = avg_val_loss if early_stopping_monitor == 'val_loss' else avg_train_loss\n",
    "            \n",
    "            # Check if there's improvement\n",
    "            if current_loss < best_loss - early_stopping_min_delta:\n",
    "                best_loss = current_loss\n",
    "                best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "                # Save best model state\n",
    "                best_model_state = {k: v.cpu().clone() for k, v in network.state_dict().items()}\n",
    "                if verbose and epoch > 0:\n",
    "                    print(f\"  → New best {early_stopping_monitor}: {best_loss:.6f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if verbose and patience_counter > 0 and (epoch + 1) % max(1, n_epochs // 10) == 0:\n",
    "                    print(f\"  → No improvement for {patience_counter} epoch(s)\")\n",
    "            \n",
    "            # Check if we should stop\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                if verbose:\n",
    "                    print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "                    print(f\"Best {early_stopping_monitor}: {best_loss:.6f} at epoch {best_epoch + 1}\")\n",
    "                \n",
    "                # Restore best model state\n",
    "                if best_model_state is not None:\n",
    "                    network.load_state_dict(best_model_state)\n",
    "                    if verbose:\n",
    "                        print(\"Restored best model weights\")\n",
    "                \n",
    "                break\n",
    "    \n",
    "    # Get final parameters\n",
    "    params_optimized = network.get_flat_params()\n",
    "    \n",
    "    # Optionally save the trained model parameters\n",
    "    if save_model:\n",
    "        if save_model_path is None:\n",
    "            if tensorboard_logdir is not None:\n",
    "                save_model_path = os.path.join(tensorboard_logdir, 'model_state_dict.pt')\n",
    "            else:\n",
    "                save_model_path = os.path.join(os.getcwd(), 'model_state_dict.pt')\n",
    "        try:\n",
    "            network.save(save_model_path)\n",
    "            if verbose:\n",
    "                print(f\"Saved model state_dict to: {save_model_path}\")\n",
    "        except Exception:\n",
    "            warnings.warn(f\"Failed to save model to {save_model_path}\")\n",
    "\n",
    "    # Add early stopping info to history\n",
    "    if early_stopping:\n",
    "        history['early_stopping'] = {\n",
    "            'triggered': patience_counter >= early_stopping_patience,\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_loss': best_loss,\n",
    "            'monitored_metric': early_stopping_monitor,\n",
    "            'patience': early_stopping_patience,\n",
    "            'final_epoch': epoch\n",
    "        }\n",
    "\n",
    "    # Print final losses\n",
    "    if verbose:\n",
    "        print(f\"\\nFinal Training Loss: {history['train_loss'][-1]:.6f}\")\n",
    "        if X_val is not None and history['val_loss']:\n",
    "            print(f\"Final Validation Loss: {history['val_loss'][-1]:.6f}\")\n",
    "        if early_stopping and history.get('early_stopping', {}).get('triggered', False):\n",
    "            print(f\"\\nEarly stopping was triggered:\")\n",
    "            print(f\"  Best {early_stopping_monitor}: {best_loss:.6f} at epoch {best_epoch + 1}\")\n",
    "            print(f\"  Training stopped at epoch {epoch + 1}\")\n",
    "\n",
    "    return params_optimized, history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
